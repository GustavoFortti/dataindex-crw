[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "isExtraImport": true,
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "importlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib",
        "description": "importlib",
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "roc_auc_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "KFold",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "XGBClassifier",
        "importPath": "xgboost",
        "description": "xgboost",
        "isExtraImport": true,
        "detail": "xgboost",
        "documentation": {}
    },
    {
        "label": "read_and_stack_csvs_dataframes",
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "isExtraImport": true,
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "drop_duplicates_for_columns",
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "isExtraImport": true,
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "read_and_stack_csvs_dataframes",
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "isExtraImport": true,
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "read_and_stack_historical_csvs_dataframes",
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "isExtraImport": true,
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "create_or_read_df",
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "isExtraImport": true,
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "create_or_read_df",
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "isExtraImport": true,
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "create_or_read_df",
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "isExtraImport": true,
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "read_and_stack_historical_csvs_dataframes",
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "isExtraImport": true,
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "calc_string_diff_in_df_col",
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "isExtraImport": true,
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "read_and_stack_historical_csvs_dataframes",
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "isExtraImport": true,
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "create_directory_if_not_exists",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "DATE_FORMAT",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "create_directory_if_not_exists",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "create_directory_if_not_exists",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "path_exists",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "DATE_FORMAT",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "create_directory_if_not_exists",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "delete_directory_and_contents",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "delete_file",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "get_old_files_by_percent",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "list_directory",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "read_json",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "save_images",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "delete_file",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "path_exists",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "read_file",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "create_directory_if_not_exists",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "list_directory",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "path_exists",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "save_file",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "create_directory_if_not_exists",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "has_files",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "path_exists",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "list_directory",
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "isExtraImport": true,
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "get_pages_with_status_true",
        "importPath": "src.lib.utils.general_functions",
        "description": "src.lib.utils.general_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.general_functions",
        "documentation": {}
    },
    {
        "label": "get_pages_with_status_true",
        "importPath": "src.lib.utils.general_functions",
        "description": "src.lib.utils.general_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.general_functions",
        "documentation": {}
    },
    {
        "label": "flatten_list",
        "importPath": "src.lib.utils.py_functions",
        "description": "src.lib.utils.py_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.py_functions",
        "documentation": {}
    },
    {
        "label": "WORDLIST",
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "isExtraImport": true,
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "WORDLIST",
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "isExtraImport": true,
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "WORDLIST",
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "isExtraImport": true,
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "BLACK_LIST",
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "isExtraImport": true,
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "get_back_words",
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "isExtraImport": true,
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "get_word_index_in_text",
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "isExtraImport": true,
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "BLACK_LIST",
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "isExtraImport": true,
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "date",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "date",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "extract",
        "importPath": "src.lib.extract.extract",
        "description": "src.lib.extract.extract",
        "isExtraImport": true,
        "detail": "src.lib.extract.extract",
        "documentation": {}
    },
    {
        "label": "products_metadata_update_old_pages_by_ref",
        "importPath": "src.lib.extract.extract",
        "description": "src.lib.extract.extract",
        "isExtraImport": true,
        "detail": "src.lib.extract.extract",
        "documentation": {}
    },
    {
        "label": "load",
        "importPath": "src.lib.load.load",
        "description": "src.lib.load.load",
        "isExtraImport": true,
        "detail": "src.lib.load.load",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "transform",
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "src.lib.extract.selenium_service",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "src.lib.extract.selenium_service",
        "description": "src.lib.extract.selenium_service",
        "detail": "src.lib.extract.selenium_service",
        "documentation": {}
    },
    {
        "label": "src.lib.utils.data_quality",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "src.lib.utils.data_quality",
        "description": "src.lib.utils.data_quality",
        "detail": "src.lib.utils.data_quality",
        "documentation": {}
    },
    {
        "label": "status_tag",
        "importPath": "src.lib.utils.data_quality",
        "description": "src.lib.utils.data_quality",
        "isExtraImport": true,
        "detail": "src.lib.utils.data_quality",
        "documentation": {}
    },
    {
        "label": "is_price",
        "importPath": "src.lib.utils.data_quality",
        "description": "src.lib.utils.data_quality",
        "isExtraImport": true,
        "detail": "src.lib.utils.data_quality",
        "documentation": {}
    },
    {
        "label": "clean_string_break_line",
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "generate_hash",
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "find_in_text_with_wordlist",
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "find_in_text_with_wordlist",
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "remove_spaces",
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "DATE_FORMAT",
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "DATE_FORMAT",
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "levenshtein",
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "crawler",
        "importPath": "src.lib.extract.crawler",
        "description": "src.lib.extract.crawler",
        "isExtraImport": true,
        "detail": "src.lib.extract.crawler",
        "documentation": {}
    },
    {
        "label": "Page",
        "importPath": "src.lib.extract.page_elements",
        "description": "src.lib.extract.page_elements",
        "isExtraImport": true,
        "detail": "src.lib.extract.page_elements",
        "documentation": {}
    },
    {
        "label": "Page",
        "importPath": "src.lib.extract.page_elements",
        "description": "src.lib.extract.page_elements",
        "isExtraImport": true,
        "detail": "src.lib.extract.page_elements",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "UserAgent",
        "importPath": "fake_useragent",
        "description": "fake_useragent",
        "isExtraImport": true,
        "detail": "fake_useragent",
        "documentation": {}
    },
    {
        "label": "UserAgent",
        "importPath": "fake_useragent",
        "description": "fake_useragent",
        "isExtraImport": true,
        "detail": "fake_useragent",
        "documentation": {}
    },
    {
        "label": "webdriver",
        "importPath": "selenium",
        "description": "selenium",
        "isExtraImport": true,
        "detail": "selenium",
        "documentation": {}
    },
    {
        "label": "TimeoutException",
        "importPath": "selenium.common.exceptions",
        "description": "selenium.common.exceptions",
        "isExtraImport": true,
        "detail": "selenium.common.exceptions",
        "documentation": {}
    },
    {
        "label": "WebDriverException",
        "importPath": "selenium.common.exceptions",
        "description": "selenium.common.exceptions",
        "isExtraImport": true,
        "detail": "selenium.common.exceptions",
        "documentation": {}
    },
    {
        "label": "Service",
        "importPath": "selenium.webdriver.chrome.service",
        "description": "selenium.webdriver.chrome.service",
        "isExtraImport": true,
        "detail": "selenium.webdriver.chrome.service",
        "documentation": {}
    },
    {
        "label": "ActionChains",
        "importPath": "selenium.webdriver.common.action_chains",
        "description": "selenium.webdriver.common.action_chains",
        "isExtraImport": true,
        "detail": "selenium.webdriver.common.action_chains",
        "documentation": {}
    },
    {
        "label": "By",
        "importPath": "selenium.webdriver.common.by",
        "description": "selenium.webdriver.common.by",
        "isExtraImport": true,
        "detail": "selenium.webdriver.common.by",
        "documentation": {}
    },
    {
        "label": "ChromeDriverManager",
        "importPath": "webdriver_manager.chrome",
        "description": "webdriver_manager.chrome",
        "isExtraImport": true,
        "detail": "webdriver_manager.chrome",
        "documentation": {}
    },
    {
        "label": "expected_conditions",
        "importPath": "selenium.webdriver.support",
        "description": "selenium.webdriver.support",
        "isExtraImport": true,
        "detail": "selenium.webdriver.support",
        "documentation": {}
    },
    {
        "label": "WebDriverWait",
        "importPath": "selenium.webdriver.support.ui",
        "description": "selenium.webdriver.support.ui",
        "isExtraImport": true,
        "detail": "selenium.webdriver.support.ui",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "parse_qs",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "BASE_URL",
        "importPath": "src.config.setup.shopify",
        "description": "src.config.setup.shopify",
        "isExtraImport": true,
        "detail": "src.config.setup.shopify",
        "documentation": {}
    },
    {
        "label": "HEADERS",
        "importPath": "src.config.setup.shopify",
        "description": "src.config.setup.shopify",
        "isExtraImport": true,
        "detail": "src.config.setup.shopify",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "process_and_ingest_products",
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "isExtraImport": true,
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "apply_generic_filters",
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "create_price_discount_percent_col",
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "create_product_def_cols",
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "create_quantity_column",
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "filter_nulls",
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "image_processing",
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "remove_blacklisted_products",
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "isExtraImport": true,
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "imagehash",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "imagehash",
        "description": "imagehash",
        "detail": "imagehash",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "load_product_definition",
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "isExtraImport": true,
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "calculate_precise_image_hash",
        "importPath": "src.lib.utils.image_functions",
        "description": "src.lib.utils.image_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.image_functions",
        "documentation": {}
    },
    {
        "label": "convert_image",
        "importPath": "src.lib.utils.image_functions",
        "description": "src.lib.utils.image_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.image_functions",
        "documentation": {}
    },
    {
        "label": "check_url_existence",
        "importPath": "src.lib.utils.web_functions",
        "description": "src.lib.utils.web_functions",
        "isExtraImport": true,
        "detail": "src.lib.utils.web_functions",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "unicodedata",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unicodedata",
        "description": "unicodedata",
        "detail": "unicodedata",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "configure_display",
        "importPath": "src.config.setup.display",
        "description": "src.config.setup.display",
        "isExtraImport": true,
        "detail": "src.config.setup.display",
        "documentation": {}
    },
    {
        "label": "configure_image_server",
        "importPath": "src.config.setup.image_server",
        "description": "src.config.setup.image_server",
        "isExtraImport": true,
        "detail": "src.config.setup.image_server",
        "documentation": {}
    },
    {
        "label": "get_used_displays",
        "kind": 2,
        "importPath": "src.config.setup.display",
        "description": "src.config.setup.display",
        "peekOfCode": "def get_used_displays():\n    \"\"\"\n    Retorna um conjunto de números de display atualmente em uso.\n    \"\"\"\n    try:\n        # Executa o comando ps aux\n        process = subprocess.Popen(['ps', 'aux'], stdout=subprocess.PIPE)\n        # Filtra os processos que contêm 'Xvfb'\n        grep_process = subprocess.Popen(['grep', 'Xvfb'], stdin=process.stdout, stdout=subprocess.PIPE)\n        process.stdout.close()  # Fecha a entrada do pipe",
        "detail": "src.config.setup.display",
        "documentation": {}
    },
    {
        "label": "find_available_display",
        "kind": 2,
        "importPath": "src.config.setup.display",
        "description": "src.config.setup.display",
        "peekOfCode": "def find_available_display(start=99, end=200):\n    \"\"\"\n    Encontra um número de display disponível entre start e end.\n    Retorna a string do display (por exemplo, ':99') ou None se não encontrar.\n    \"\"\"\n    used_displays = get_used_displays()\n    for display_num in range(start, end):\n        if display_num not in used_displays:\n            return f\":{display_num}\"\n    return None",
        "detail": "src.config.setup.display",
        "documentation": {}
    },
    {
        "label": "start_xvfb",
        "kind": 2,
        "importPath": "src.config.setup.display",
        "description": "src.config.setup.display",
        "peekOfCode": "def start_xvfb(display):\n    \"\"\"\n    Inicia o Xvfb no display especificado.\n    Retorna o objeto subprocess.Popen ou None se falhar.\n    \"\"\"\n    try:\n        xvfb_command = ['Xvfb', display, '-screen', '0', '1280x720x24']\n        xvfb_process = subprocess.Popen(xvfb_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        # Aguarda um pouco para garantir que o Xvfb inicie\n        time.sleep(2)",
        "detail": "src.config.setup.display",
        "documentation": {}
    },
    {
        "label": "terminate_process",
        "kind": 2,
        "importPath": "src.config.setup.display",
        "description": "src.config.setup.display",
        "peekOfCode": "def terminate_process(process):\n    \"\"\"\n    Termina o processo fornecido de forma graciosa, forçando se necessário.\n    \"\"\"\n    try:\n        if process.poll() is None:\n            process.terminate()\n            try:\n                process.wait(timeout=5)\n                message(f\"Processo {process.pid} encerrado.\")",
        "detail": "src.config.setup.display",
        "documentation": {}
    },
    {
        "label": "configure_display_and_test_chrome",
        "kind": 2,
        "importPath": "src.config.setup.display",
        "description": "src.config.setup.display",
        "peekOfCode": "def configure_display_and_test_chrome():\n    \"\"\"\n    Encontra um display disponível, inicia o Xvfb, configura a variável DISPLAY,\n    lança o Chrome em modo headless e verifica se funciona.\n    Retorna True se bem-sucedido, False caso contrário.\n    \"\"\"\n    # Encontra um display disponível\n    display = find_available_display()\n    if not display:\n        message(\"Nenhum DISPLAY disponível encontrado.\")",
        "detail": "src.config.setup.display",
        "documentation": {}
    },
    {
        "label": "configure_display",
        "kind": 2,
        "importPath": "src.config.setup.display",
        "description": "src.config.setup.display",
        "peekOfCode": "def configure_display():\n    message(\"FUNCTION CONFIGURE DISPLAY\")\n    sucesso = configure_display_and_test_chrome()\n    if sucesso:\n        message(\"Configuração do DISPLAY e teste do Chrome concluídos com sucesso.\")\n    else:\n        message(\"Falha na configuração do DISPLAY e/ou no teste do Chrome.\")",
        "detail": "src.config.setup.display",
        "documentation": {}
    },
    {
        "label": "configure_image_server",
        "kind": 2,
        "importPath": "src.config.setup.image_server",
        "description": "src.config.setup.image_server",
        "peekOfCode": "def configure_image_server():\n    os.environ['DATAINDEX_IMG_PATH'] = os.path.join(LOCAL, '..', 'image-server')\n    os.environ['DATAINDEX_IMG_URL'] = GIT_CONF[\"dataindex_img_url\"]\n    img_path = os.environ.get('DATAINDEX_IMG_PATH')\n    if os.path.isdir(img_path):\n        message(f\"{img_path} Ok\")\n    else:\n        clone_repository(GIT_CONF[\"remote_url\"])\n        setup_git_config(img_path, GIT_CONF['username'], GIT_CONF['email'], GIT_CONF['remote_name'], GIT_CONF['remote_url'])\ndef clone_repository(remote_url):",
        "detail": "src.config.setup.image_server",
        "documentation": {}
    },
    {
        "label": "clone_repository",
        "kind": 2,
        "importPath": "src.config.setup.image_server",
        "description": "src.config.setup.image_server",
        "peekOfCode": "def clone_repository(remote_url):\n    try:\n        original_dir = os.getcwd()\n        os.chdir(LOCAL + \"/..\")\n        subprocess.check_call(['git', 'clone', remote_url])\n        message(f\"Repositório clonado em {LOCAL}\")\n    except subprocess.CalledProcessError as e:\n        message(f\"Erro ao executar o comando Git: {e}\")\n    except Exception as e:\n        message(f\"Erro ao clonar o repositório: {e}\")",
        "detail": "src.config.setup.image_server",
        "documentation": {}
    },
    {
        "label": "setup_git_config",
        "kind": 2,
        "importPath": "src.config.setup.image_server",
        "description": "src.config.setup.image_server",
        "peekOfCode": "def setup_git_config(project_dir, username, email, remote_name, remote_url):\n    try:\n        if not os.path.exists(project_dir):\n            os.makedirs(project_dir)\n        original_dir = os.getcwd()\n        os.chdir(project_dir)\n        subprocess.check_call(['git', 'config', 'init.defaultBranch', 'master'])\n        subprocess.check_call(['git', 'config', 'user.name', username])\n        subprocess.check_call(['git', 'config', 'user.email', email])\n        remote_check = subprocess.check_output(['git', 'remote']).decode('utf-8').strip()",
        "detail": "src.config.setup.image_server",
        "documentation": {}
    },
    {
        "label": "LOCAL",
        "kind": 5,
        "importPath": "src.config.setup.image_server",
        "description": "src.config.setup.image_server",
        "peekOfCode": "LOCAL = os.getenv('LOCAL')\nGIT_CONF = {\n    \"username\": \"Gustavo Fortti\",\n    \"email\": \"gustavofortti@gmail.com\",\n    \"remote_name\": \"origin\",\n    \"remote_url\": \"https://github.com/GustavoFortti/dataindex-img.git\",\n    \"dataindex_img_url\": \"https://raw.githubusercontent.com/GustavoFortti/dataindex-img/master/imgs/\"\n}\ndef configure_image_server():\n    os.environ['DATAINDEX_IMG_PATH'] = os.path.join(LOCAL, '..', 'image-server')",
        "detail": "src.config.setup.image_server",
        "documentation": {}
    },
    {
        "label": "GIT_CONF",
        "kind": 5,
        "importPath": "src.config.setup.image_server",
        "description": "src.config.setup.image_server",
        "peekOfCode": "GIT_CONF = {\n    \"username\": \"Gustavo Fortti\",\n    \"email\": \"gustavofortti@gmail.com\",\n    \"remote_name\": \"origin\",\n    \"remote_url\": \"https://github.com/GustavoFortti/dataindex-img.git\",\n    \"dataindex_img_url\": \"https://raw.githubusercontent.com/GustavoFortti/dataindex-img/master/imgs/\"\n}\ndef configure_image_server():\n    os.environ['DATAINDEX_IMG_PATH'] = os.path.join(LOCAL, '..', 'image-server')\n    os.environ['DATAINDEX_IMG_URL'] = GIT_CONF[\"dataindex_img_url\"]",
        "detail": "src.config.setup.image_server",
        "documentation": {}
    },
    {
        "label": "ACCESS_TOKEN",
        "kind": 5,
        "importPath": "src.config.setup.shopify",
        "description": "src.config.setup.shopify",
        "peekOfCode": "ACCESS_TOKEN = \"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\"\nSHOP_NAME = \"2ef5d6-14\"\nAPI_VERSION = \"2024-07\"\nBASE_URL = f\"https://{SHOP_NAME}.myshopify.com/admin/api/{API_VERSION}/\"\nHEADERS = {\n    \"Content-Type\": \"application/json\",\n    \"X-Shopify-Access-Token\": ACCESS_TOKEN\n}",
        "detail": "src.config.setup.shopify",
        "documentation": {}
    },
    {
        "label": "SHOP_NAME",
        "kind": 5,
        "importPath": "src.config.setup.shopify",
        "description": "src.config.setup.shopify",
        "peekOfCode": "SHOP_NAME = \"2ef5d6-14\"\nAPI_VERSION = \"2024-07\"\nBASE_URL = f\"https://{SHOP_NAME}.myshopify.com/admin/api/{API_VERSION}/\"\nHEADERS = {\n    \"Content-Type\": \"application/json\",\n    \"X-Shopify-Access-Token\": ACCESS_TOKEN\n}",
        "detail": "src.config.setup.shopify",
        "documentation": {}
    },
    {
        "label": "API_VERSION",
        "kind": 5,
        "importPath": "src.config.setup.shopify",
        "description": "src.config.setup.shopify",
        "peekOfCode": "API_VERSION = \"2024-07\"\nBASE_URL = f\"https://{SHOP_NAME}.myshopify.com/admin/api/{API_VERSION}/\"\nHEADERS = {\n    \"Content-Type\": \"application/json\",\n    \"X-Shopify-Access-Token\": ACCESS_TOKEN\n}",
        "detail": "src.config.setup.shopify",
        "documentation": {}
    },
    {
        "label": "BASE_URL",
        "kind": 5,
        "importPath": "src.config.setup.shopify",
        "description": "src.config.setup.shopify",
        "peekOfCode": "BASE_URL = f\"https://{SHOP_NAME}.myshopify.com/admin/api/{API_VERSION}/\"\nHEADERS = {\n    \"Content-Type\": \"application/json\",\n    \"X-Shopify-Access-Token\": ACCESS_TOKEN\n}",
        "detail": "src.config.setup.shopify",
        "documentation": {}
    },
    {
        "label": "HEADERS",
        "kind": 5,
        "importPath": "src.config.setup.shopify",
        "description": "src.config.setup.shopify",
        "peekOfCode": "HEADERS = {\n    \"Content-Type\": \"application/json\",\n    \"X-Shopify-Access-Token\": ACCESS_TOKEN\n}",
        "detail": "src.config.setup.shopify",
        "documentation": {}
    },
    {
        "label": "set_conf",
        "kind": 2,
        "importPath": "src.jobs.data_intelligence.product_definition.job",
        "description": "src.jobs.data_intelligence.product_definition.job",
        "peekOfCode": "def set_conf(args, local):\n    conf = {}\n    conf[\"local\"] = local\n    conf[\"job_name\"] = args.job_name\n    conf[\"page_name\"] = args.page_name\n    conf[\"page_type\"] = args.page_type\n    conf[\"country\"] = args.country\n    conf[\"src_data_path\"] = f\"{local}/data/{conf['page_type']}/{conf['country']}\"\n    conf[\"data_path\"] = f\"{conf['src_data_path']}/product_definition\"\n    conf[\"wordlist\"] = WORDLIST[conf['page_type']]",
        "detail": "src.jobs.data_intelligence.product_definition.job",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "src.jobs.data_intelligence.product_definition.job",
        "description": "src.jobs.data_intelligence.product_definition.job",
        "peekOfCode": "def run(args):\n    global DATA_PATH\n    global WORDLIST_CONF\n    global LOCAL\n    LOCAL = os.getenv('LOCAL')\n    conf = set_conf(args, LOCAL)\n    print(\"JOB_NAME: \" + conf[\"job_name\"])\n    conf.update(vars(args))\n    src_data_path = conf[\"src_data_path\"]\n    WORDLIST_CONF = conf['wordlist']",
        "detail": "src.jobs.data_intelligence.product_definition.job",
        "documentation": {}
    },
    {
        "label": "add_synonyms_to_cols_with_wordlist",
        "kind": 2,
        "importPath": "src.jobs.data_intelligence.product_definition.job",
        "description": "src.jobs.data_intelligence.product_definition.job",
        "peekOfCode": "def add_synonyms_to_cols_with_wordlist(df, col, conf):\n    for idx, row in df.iterrows():\n        ref = row['ref']\n        synonyms = []\n        synonym = []\n        for key, value in WORDLIST_CONF.items():\n            words = value['subject']\n            tag = value[conf[\"country\"]]\n            aux_subject = [words for subject in row['subject'].split(\", \") if subject in words]\n            if (aux_subject != []):",
        "detail": "src.jobs.data_intelligence.product_definition.job",
        "documentation": {}
    },
    {
        "label": "calc_def_product",
        "kind": 2,
        "importPath": "src.jobs.data_intelligence.product_definition.job",
        "description": "src.jobs.data_intelligence.product_definition.job",
        "peekOfCode": "def calc_def_product(df):\n    df_filtered = df[(df['product_predicted'] != 0) & \n                    (df['probability_class_1'] >= 0.7)]\n    def filter_document_index(df):\n        if ((len(df) == 1) or (df['document_index'].max() == df['document_index'].min())):\n            return df\n        return df[df['document_index'] != df['document_index'].max()]\n    df_filtered = df_filtered.groupby('ref', as_index=False).apply(filter_document_index).reset_index(drop=True)\n    df_grouped = df_filtered.groupby(['ref', 'subject_encoded']).agg({\n        'location': 'sum',",
        "detail": "src.jobs.data_intelligence.product_definition.job",
        "documentation": {}
    },
    {
        "label": "unique_subject",
        "kind": 2,
        "importPath": "src.jobs.data_intelligence.product_definition.job",
        "description": "src.jobs.data_intelligence.product_definition.job",
        "peekOfCode": "def unique_subject(subjects):\n    unique_subjects = set()\n    for subject in subjects:\n        cleaned_subject = subject.strip()\n        unique_subjects.add(cleaned_subject)\n    return ' '.join(unique_subjects)\ndef run_score_train(df):\n    X = df.drop(['target', 'ref'], axis=1)\n    y = df['target']\n    model = train_and_evaluate_models(X, y)",
        "detail": "src.jobs.data_intelligence.product_definition.job",
        "documentation": {}
    },
    {
        "label": "run_score_train",
        "kind": 2,
        "importPath": "src.jobs.data_intelligence.product_definition.job",
        "description": "src.jobs.data_intelligence.product_definition.job",
        "peekOfCode": "def run_score_train(df):\n    X = df.drop(['target', 'ref'], axis=1)\n    y = df['target']\n    model = train_and_evaluate_models(X, y)\n    return model\ndef run_score_predict(df, model):\n    df_temp = df.copy()\n    X_predict = df_temp.drop(['target', 'ref'], axis=1)\n    y_pred = model.predict(X_predict)\n    y_probability = model.predict_proba(X_predict)",
        "detail": "src.jobs.data_intelligence.product_definition.job",
        "documentation": {}
    },
    {
        "label": "run_score_predict",
        "kind": 2,
        "importPath": "src.jobs.data_intelligence.product_definition.job",
        "description": "src.jobs.data_intelligence.product_definition.job",
        "peekOfCode": "def run_score_predict(df, model):\n    df_temp = df.copy()\n    X_predict = df_temp.drop(['target', 'ref'], axis=1)\n    y_pred = model.predict(X_predict)\n    y_probability = model.predict_proba(X_predict)\n    df['product_predicted'] = y_pred\n    df['probability_class_0'] = y_probability[:, 0]\n    df['probability_class_1'] = y_probability[:, 1]\n    return df[['ref', 'location', 'document_index', 'subject_encoded', 'product_predicted', 'probability_class_0', 'probability_class_1']]\ndef train_and_evaluate_models(X, y, n_splits=5):",
        "detail": "src.jobs.data_intelligence.product_definition.job",
        "documentation": {}
    },
    {
        "label": "train_and_evaluate_models",
        "kind": 2,
        "importPath": "src.jobs.data_intelligence.product_definition.job",
        "description": "src.jobs.data_intelligence.product_definition.job",
        "peekOfCode": "def train_and_evaluate_models(X, y, n_splits=5):\n    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=200, random_state=42)\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    accuracies = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n    aucs = []\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]",
        "detail": "src.jobs.data_intelligence.product_definition.job",
        "documentation": {}
    },
    {
        "label": "data_prep",
        "kind": 2,
        "importPath": "src.jobs.data_intelligence.product_definition.job",
        "description": "src.jobs.data_intelligence.product_definition.job",
        "peekOfCode": "def data_prep(df, conf):\n    df = df.drop_duplicates()\n    subjects = []\n    for items in WORDLIST_CONF.values():\n        subjects.append(items['subject'])\n    subjects = np.array(flatten_list(subjects))\n    message(\"read dictionaries\")\n    dictionaries = importlib.import_module(f\"src.lib.wordlist.{conf[\"country\"]}.dictionary\").get_dictionary(LOCAL)\n    brands = np.array(list(set(df['brand'].values)))\n    dictionaries = np.unique(np.concatenate([dictionaries, subjects, brands]))",
        "detail": "src.jobs.data_intelligence.product_definition.job",
        "documentation": {}
    },
    {
        "label": "create_new_features",
        "kind": 2,
        "importPath": "src.jobs.data_intelligence.product_definition.job",
        "description": "src.jobs.data_intelligence.product_definition.job",
        "peekOfCode": "def create_new_features(df):\n    # Grouping by columns 'ref', 'subject', and 'document_index'\n    cols = ['ref', 'subject', 'document_index']\n    # Aggregating 'subject_encoded' by count and 'location' by taking the mean of the smallest 20% or 1 observation\n    grouped_df = df.groupby(cols).agg({\n        'subject_encoded': 'count',  # Counting occurrences of 'subject_encoded'\n        'location': (lambda x: x.nsmallest(max(int(len(x) * 0.20), 1)).mean()),  # Calculating the mean of the smallest 20% or 1 observation of 'location'\n    }).reset_index()\n    # Renaming columns for clarity\n    grouped_df = grouped_df[['ref', 'subject', 'document_index', 'subject_encoded', 'location']]",
        "detail": "src.jobs.data_intelligence.product_definition.job",
        "documentation": {}
    },
    {
        "label": "convert_type_columns",
        "kind": 2,
        "importPath": "src.jobs.data_intelligence.product_definition.job",
        "description": "src.jobs.data_intelligence.product_definition.job",
        "peekOfCode": "def convert_type_columns(df, cols, cols_types):\n    for col in cols:\n        if cols_types == int:\n            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(float).astype(int)\n        else:\n            df[col] = df[col].astype(cols_types)\n    return df\ndef normalize_words(df, columns, dictionaries, words):\n    mask = np.isin(dictionaries, words)\n    filtered_data_set = dictionaries[mask]",
        "detail": "src.jobs.data_intelligence.product_definition.job",
        "documentation": {}
    },
    {
        "label": "normalize_words",
        "kind": 2,
        "importPath": "src.jobs.data_intelligence.product_definition.job",
        "description": "src.jobs.data_intelligence.product_definition.job",
        "peekOfCode": "def normalize_words(df, columns, dictionaries, words):\n    mask = np.isin(dictionaries, words)\n    filtered_data_set = dictionaries[mask]\n    filtered_data_set = np.insert(filtered_data_set, 0, \"0\")\n    for col in columns:\n        df[f\"{col}_encoded\"] = df[col].apply(lambda x: np.where(filtered_data_set == x)[0][0] if x in filtered_data_set else x)\n        df[f\"{col}_encoded\"] = df[f\"{col}_encoded\"].where(~df[col].isnull(), other=\"0\")\n        df[f\"{col}_encoded\"] = df[f\"{col}_encoded\"].replace(regex=r'.*\\D+.*', value='-1', inplace=False)\n    df = df.astype(str)\n    return df",
        "detail": "src.jobs.data_intelligence.product_definition.job",
        "documentation": {}
    },
    {
        "label": "set_conf",
        "kind": 2,
        "importPath": "src.jobs.data_shelf.history_price.job",
        "description": "src.jobs.data_shelf.history_price.job",
        "peekOfCode": "def set_conf(args, local):\n    conf = {}\n    conf[\"local\"] = local\n    conf[\"job_name\"] = args.job_name\n    conf[\"page_name\"] = args.page_name\n    conf[\"page_type\"] = args.page_type\n    conf[\"country\"] = args.country\n    conf[\"src_data_path\"] = f\"{local}/data/{conf['page_type']}/{conf['country']}\"\n    conf[\"data_path\"] = f\"{conf['src_data_path']}/history_price\"\n    conf[\"pages_path\"] = f\"{local}/src/jobs/slave_page/pages/{conf['country']}\"",
        "detail": "src.jobs.data_shelf.history_price.job",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "src.jobs.data_shelf.history_price.job",
        "description": "src.jobs.data_shelf.history_price.job",
        "peekOfCode": "def run(args):\n    global LOCAL\n    LOCAL = os.getenv('LOCAL')\n    conf = set_conf(args, LOCAL)\n    print(\"JOB_NAME: \" + conf[\"job_name\"])\n    conf.update(vars(args))\n    src_data_path = conf[\"src_data_path\"]\n    data_path = conf['data_path']\n    create_directory_if_not_exists(data_path)\n    message(\"read data\")",
        "detail": "src.jobs.data_shelf.history_price.job",
        "documentation": {}
    },
    {
        "label": "set_conf",
        "kind": 2,
        "importPath": "src.jobs.master_page.master.job",
        "description": "src.jobs.master_page.master.job",
        "peekOfCode": "def set_conf(args, local):\n    conf = {}\n    conf[\"local\"] = local\n    conf[\"job_name\"] = args.job_name\n    conf[\"page_name\"] = args.page_name\n    conf[\"page_type\"] = args.page_type\n    conf[\"country\"] = args.country\n    conf[\"src_data_path\"] = f\"{local}/data/{conf['page_type']}/{conf['country']}\"\n    conf[\"wordlist\"] = WORDLIST[conf['page_type']]\n    return conf",
        "detail": "src.jobs.master_page.master.job",
        "documentation": {}
    },
    {
        "label": "update_conf_with_page_config",
        "kind": 2,
        "importPath": "src.jobs.master_page.master.job",
        "description": "src.jobs.master_page.master.job",
        "peekOfCode": "def update_conf_with_page_config(conf, page_conf, local, args):\n    conf[\"name\"] = page_conf.JOB_NAME\n    conf[\"brand\"] = page_conf.BRAND\n    conf[\"url\"] = page_conf.URL\n    conf[\"product_definition_tag_map\"] = page_conf.PRODUCT_DEFINITION_TAG_MAP\n    conf[\"dynamic_scroll\"] = page_conf.DYNAMIC_SCROLL\n    conf[\"data_path\"] = f\"{conf['src_data_path']}/{page_conf.JOB_NAME}\"\n    conf[\"seed_path\"] = f\"{local}/src/jobs/slave_page/pages/{conf['country']}/{page_conf.JOB_NAME}\"\n    conf[\"product_definition\"] = f\"{local}/data/{conf['page_type']}/brazil/product_definition\"\n    conf[\"scroll_page\"] = True",
        "detail": "src.jobs.master_page.master.job",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "src.jobs.master_page.master.job",
        "description": "src.jobs.master_page.master.job",
        "peekOfCode": "def run(args):\n    message(\"INICIANDO O JOB\")\n    local = os.getenv('LOCAL')\n    conf = set_conf(args, local)\n    message(\"Configuração inicial:\")\n    # message(conf)\n    module_name = f\"src.jobs.slave_page.pages.{conf['country']}.{conf['page_name']}.conf\"\n    page_conf = importlib.import_module(module_name)\n    conf = update_conf_with_page_config(conf, page_conf, local, args)\n    message(\"Configuração atualizada:\")",
        "detail": "src.jobs.master_page.master.job",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "peekOfCode": "JOB_NAME = \"adaptogen\"\nBRAND = \"adaptogen\"\nURL = \"https://adaptogen.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#tab-description'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "peekOfCode": "BRAND = \"adaptogen\"\nURL = \"https://adaptogen.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#tab-description'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "peekOfCode": "URL = \"https://adaptogen.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#tab-description'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#tab-description'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#tab-description'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.dry",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.dry",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('li', class_='product-type-simple')\n    return items\ndef get_product_url(conf, soup):\n    product_link_element = soup.find('a')\n    return product_link_element['href'] if product_link_element else None\ndef get_title(conf, soup):\n    title_element = soup.find('h2', class_='woocommerce-loop-product__title')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_element = soup.find('a')\n    return product_link_element['href'] if product_link_element else None\ndef get_title(conf, soup):\n    title_element = soup.find('h2', class_='woocommerce-loop-product__title')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    price_container = soup.find(class_=\"a_vista\")\n    if (price_container):\n        price_element = price_container.find('p')",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('h2', class_='woocommerce-loop-product__title')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    price_container = soup.find(class_=\"a_vista\")\n    if (price_container):\n        price_element = price_container.find('p')\n        price = price_element.contents[0].strip()\n    return price\ndef get_image_url(conf, soup):",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    price_container = soup.find(class_=\"a_vista\")\n    if (price_container):\n        price_element = price_container.find('p')\n        price = price_element.contents[0].strip()\n    return price\ndef get_image_url(conf, soup):\n    image_element = soup.find('img', class_='entered lazyloaded')\n    return image_element['src'] if image_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    image_element = soup.find('img', class_='entered lazyloaded')\n    return image_element['src'] if image_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.adaptogen.page_url",
        "description": "src.jobs.slave_page.pages.brazil.adaptogen.page_url",
        "peekOfCode": "def get_url(conf, url):\n    conf[\"index\"] = None\n    return url",
        "detail": "src.jobs.slave_page.pages.brazil.adaptogen.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "peekOfCode": "JOB_NAME = \"atlhetica_nutrition\"\nBRAND = \"atlhetica nutrition\"\nURL = \"https://www.atlheticanutrition.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div:nth-child(1) > div > section > div > div > div > div'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.8,\n    \"scroll_step\": 500,",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "peekOfCode": "BRAND = \"atlhetica nutrition\"\nURL = \"https://www.atlheticanutrition.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div:nth-child(1) > div > section > div > div > div > div'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.8,\n    \"scroll_step\": 500,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "peekOfCode": "URL = \"https://www.atlheticanutrition.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div:nth-child(1) > div > section > div > div > div > div'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.8,\n    \"scroll_step\": 500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div:nth-child(1) > div > section > div > div > div > div'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.8,\n    \"scroll_step\": 500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div:nth-child(1) > div > section > div > div > div > div'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.8,\n    \"scroll_step\": 500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.8,\n    \"scroll_step\": 500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.dry",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.dry",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('div', class_='vtex-search-result-3-x-galleryItem')\n    return items\ndef get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='vtex-product-summary-2-x-clearLink')\n    return conf[\"url\"] + product_link_element['href'] if product_link_element else None\ndef get_title(conf, soup):\n    title_element = soup.find('span', class_='vtex-product-summary-2-x-brandName')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='vtex-product-summary-2-x-clearLink')\n    return conf[\"url\"] + product_link_element['href'] if product_link_element else None\ndef get_title(conf, soup):\n    title_element = soup.find('span', class_='vtex-product-summary-2-x-brandName')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    # Busca pelo elemento que contém o preço de venda não riscado\n    selling_price_element = soup.find(class_=\"vtex-store-components-3-x-sellingPriceValue\")\n    if not selling_price_element:",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('span', class_='vtex-product-summary-2-x-brandName')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    # Busca pelo elemento que contém o preço de venda não riscado\n    selling_price_element = soup.find(class_=\"vtex-store-components-3-x-sellingPriceValue\")\n    if not selling_price_element:\n        # Se não encontrar o preço de venda, tenta buscar por qualquer preço disponível\n        selling_price_element = soup.find(class_=\"vtex-product-summary-2-x-currencyContainer\")\n    if selling_price_element:",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    # Busca pelo elemento que contém o preço de venda não riscado\n    selling_price_element = soup.find(class_=\"vtex-store-components-3-x-sellingPriceValue\")\n    if not selling_price_element:\n        # Se não encontrar o preço de venda, tenta buscar por qualquer preço disponível\n        selling_price_element = soup.find(class_=\"vtex-product-summary-2-x-currencyContainer\")\n    if selling_price_element:\n        # Captura as diferentes partes do preço\n        currency_code = selling_price_element.find(class_=\"vtex-product-summary-2-x-currencyCode\").get_text(strip=True)\n        currency_integer = selling_price_element.find(class_=\"vtex-product-summary-2-x-currencyInteger\").get_text(strip=True)",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    image_element = soup.find('img', class_='vtex-product-summary-2-x-imageNormal')\n    return image_element['src'] if image_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_url",
        "description": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(conf[\"index\"])\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.atlhetica_nutrition.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "peekOfCode": "JOB_NAME = \"black_skull\"\nBRAND = \"black skull\"\nURL = \"https://www.blackskullusa.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div:nth-child(1) > div.vtex-tab-layout-0-x-container > div.vtex-tab-layout-0-x-contentContainer.w-100'}    \n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "peekOfCode": "BRAND = \"black skull\"\nURL = \"https://www.blackskullusa.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div:nth-child(1) > div.vtex-tab-layout-0-x-container > div.vtex-tab-layout-0-x-contentContainer.w-100'}    \n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "peekOfCode": "URL = \"https://www.blackskullusa.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div:nth-child(1) > div.vtex-tab-layout-0-x-container > div.vtex-tab-layout-0-x-contentContainer.w-100'}    \n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div:nth-child(1) > div.vtex-tab-layout-0-x-container > div.vtex-tab-layout-0-x-contentContainer.w-100'}    \n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div:nth-child(1) > div.vtex-tab-layout-0-x-container > div.vtex-tab-layout-0-x-contentContainer.w-100'}    \n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.dry",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.dry",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('div', class_='vtex-search-result-3-x-galleryItem')\n    return items\ndef get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='vtex-product-summary-2-x-clearLink')\n    return conf[\"url\"] + product_link_element['href'] if product_link_element else None\ndef get_title(conf, soup):\n    title_element = soup.find('span', class_='vtex-product-summary-2-x-brandName')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='vtex-product-summary-2-x-clearLink')\n    return conf[\"url\"] + product_link_element['href'] if product_link_element else None\ndef get_title(conf, soup):\n    title_element = soup.find('span', class_='vtex-product-summary-2-x-brandName')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    price = None\n    price_element = soup.find(\"span\", class_=\"vtex-product-price-1-x-sellingPriceValue\")\n    if price_element:",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('span', class_='vtex-product-summary-2-x-brandName')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    price = None\n    price_element = soup.find(\"span\", class_=\"vtex-product-price-1-x-sellingPriceValue\")\n    if price_element:\n        currency_code = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyCode\").get_text(strip=True)\n        currency_integer = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyInteger\").get_text(strip=True)\n        currency_decimal = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyDecimal\").get_text(strip=True)",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    price = None\n    price_element = soup.find(\"span\", class_=\"vtex-product-price-1-x-sellingPriceValue\")\n    if price_element:\n        currency_code = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyCode\").get_text(strip=True)\n        currency_integer = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyInteger\").get_text(strip=True)\n        currency_decimal = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyDecimal\").get_text(strip=True)\n        currency_fraction = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyFraction\").get_text(strip=True)\n        price = f\"{currency_code} {currency_integer}{currency_decimal}{currency_fraction}\"\n    return price",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    image_element = soup.find('img', class_='vtex-product-summary-2-x-imageNormal')\n    return image_element['src'] if image_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.black_skull.page_url",
        "description": "src.jobs.slave_page.pages.brazil.black_skull.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(conf[\"index\"])\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.black_skull.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "peekOfCode": "JOB_NAME = \"boldsnacks\"\nBRAND = \"boldsnacks\"\nURL = \"https://www.boldsnacks.com.br/\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#ProductSection-template--17547208458430__main > div.productView-container.container > div > div.productView-top > div.halo-productView-right.productView-details.clearfix > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "peekOfCode": "BRAND = \"boldsnacks\"\nURL = \"https://www.boldsnacks.com.br/\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#ProductSection-template--17547208458430__main > div.productView-container.container > div > div.productView-top > div.halo-productView-right.productView-details.clearfix > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "peekOfCode": "URL = \"https://www.boldsnacks.com.br/\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#ProductSection-template--17547208458430__main > div.productView-container.container > div > div.productView-top > div.halo-productView-right.productView-details.clearfix > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#ProductSection-template--17547208458430__main > div.productView-container.container > div > div.productView-top > div.halo-productView-right.productView-details.clearfix > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#ProductSection-template--17547208458430__main > div.productView-container.container > div > div.productView-top > div.halo-productView-right.productView-details.clearfix > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.dry",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    def_string = \"barrinha \"\n    df['title'] = def_string + df['title']\n    df = transform(CONF, df)\n    df['name'] = df['name'].str.slice(len(def_string))\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.dry",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('li', class_='product collection-product-list')\n    return items\ndef get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='card-link')\n    product_link = conf[\"url\"] + product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('a', class_='card-title center link-underline card-title-ellipsis')\n    title = title_element.get_text().strip() if title_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='card-link')\n    product_link = conf[\"url\"] + product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('a', class_='card-title center link-underline card-title-ellipsis')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    def format_price(price_text):",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('a', class_='card-title center link-underline card-title-ellipsis')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    def format_price(price_text):\n        cleaned_price = price_text.strip()\n        if \",\" in cleaned_price:\n            return None\n        else:",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    def format_price(price_text):\n        cleaned_price = price_text.strip()\n        if \",\" in cleaned_price:\n            return None\n        else:\n            return cleaned_price + \",00\"\n    price_sale_span = soup.find('span', class_='price-item--sale')\n    if price_sale_span:\n        price_text = price_sale_span.get_text()",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    div_container = soup.find('div', class_='card-product__wrapper')\n    if div_container:\n        image_element = div_container.find('img')\n        if image_element and 'data-srcset' in image_element.attrs:\n            srcset = image_element['data-srcset']\n            images = srcset.split(\",\")\n            middle_image = images[len(images) // 2].split(\" \")[0].strip()\n            image_url = \"https:\" + middle_image if middle_image.startswith(\"//\") else middle_image\n            return image_url",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.boldsnacks.page_url",
        "description": "src.jobs.slave_page.pages.brazil.boldsnacks.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(conf[\"index\"])\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.boldsnacks.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "peekOfCode": "JOB_NAME = \"dark_lab\"\nBRAND = \"dark lab\"\nURL = \"https://darklabsuplementos.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#single-product > div > div:nth-child(2) > div > div.px-3'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "peekOfCode": "BRAND = \"dark lab\"\nURL = \"https://darklabsuplementos.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#single-product > div > div:nth-child(2) > div > div.px-3'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "peekOfCode": "URL = \"https://darklabsuplementos.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#single-product > div > div:nth-child(2) > div > div.px-3'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#single-product > div > div:nth-child(2) > div > div.px-3'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#single-product > div > div:nth-child(2) > div > div.px-3'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.dry",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.dry",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('div', class_='item-with-sidebar')\n    return items\ndef get_product_url(conf, soup):\n    product_link_container = soup.find('div', class_='item-image')\n    product_link_element = product_link_container.find('a') if product_link_container else None\n    product_link = product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('div', class_='js-item-name')",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_container = soup.find('div', class_='item-image')\n    product_link_element = product_link_container.find('a') if product_link_container else None\n    product_link = product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('div', class_='js-item-name')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('div', class_='js-item-name')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    price_element = soup.find('span', class_='js-price-display')\n    price = price_element.get_text().strip() if price_element else None\n    return price\ndef get_image_url(conf, soup):\n    link_imagem = None",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    price_element = soup.find('span', class_='js-price-display')\n    price = price_element.get_text().strip() if price_element else None\n    return price\ndef get_image_url(conf, soup):\n    link_imagem = None\n    image_container = soup.find('div', class_='item-image')\n    image_element = image_container.find('img') if image_container else None\n    try:\n        link_imagem = \"https:\" + image_element.get('srcset').split(' ')[0] if image_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    link_imagem = None\n    image_container = soup.find('div', class_='item-image')\n    image_element = image_container.find('img') if image_container else None\n    try:\n        link_imagem = \"https:\" + image_element.get('srcset').split(' ')[0] if image_element else None\n    except:\n        pass\n    try:\n        link_imagem = \"https:\" + image_element.get('data-srcset').split(' ')[0] if image_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dark_lab.page_url",
        "description": "src.jobs.slave_page.pages.brazil.dark_lab.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(conf[\"index\"])\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.dark_lab.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "description": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "peekOfCode": "JOB_NAME = \"darkness\"\nBRAND = \"darkness\"\nURL = \"https://www.darkness.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': 'section', 'class': 'product-information'}, \n    {'tag': 'div', 'class': '__bs-nutri-table-container'}, \n    {'tag': 'div', 'class': 'col-md-6'},\n    {'tag': 'div', 'class': '__bs-evora-list-text'}, \n    {'tag': 'div', 'id': 'descricao'},",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "description": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "peekOfCode": "BRAND = \"darkness\"\nURL = \"https://www.darkness.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': 'section', 'class': 'product-information'}, \n    {'tag': 'div', 'class': '__bs-nutri-table-container'}, \n    {'tag': 'div', 'class': 'col-md-6'},\n    {'tag': 'div', 'class': '__bs-evora-list-text'}, \n    {'tag': 'div', 'id': 'descricao'},\n    {'tag': 'div', 'id': 'sugestoes'},",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "description": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "peekOfCode": "URL = \"https://www.darkness.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': 'section', 'class': 'product-information'}, \n    {'tag': 'div', 'class': '__bs-nutri-table-container'}, \n    {'tag': 'div', 'class': 'col-md-6'},\n    {'tag': 'div', 'class': '__bs-evora-list-text'}, \n    {'tag': 'div', 'id': 'descricao'},\n    {'tag': 'div', 'id': 'sugestoes'},\n]",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "description": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': 'section', 'class': 'product-information'}, \n    {'tag': 'div', 'class': '__bs-nutri-table-container'}, \n    {'tag': 'div', 'class': 'col-md-6'},\n    {'tag': 'div', 'class': '__bs-evora-list-text'}, \n    {'tag': 'div', 'id': 'descricao'},\n    {'tag': 'div', 'id': 'sugestoes'},\n]\nDYNAMIC_SCROLL = {",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "description": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': 'section', 'class': 'product-information'}, \n    {'tag': 'div', 'class': '__bs-nutri-table-container'}, \n    {'tag': 'div', 'class': 'col-md-6'},\n    {'tag': 'div', 'class': '__bs-evora-list-text'}, \n    {'tag': 'div', 'id': 'descricao'},\n    {'tag': 'div', 'id': 'sugestoes'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "description": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.dry",
        "description": "src.jobs.slave_page.pages.brazil.darkness.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving transform\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.dry",
        "description": "src.jobs.slave_page.pages.brazil.darkness.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('div', class_='vtex-search-result-3-x-galleryItem')\n    return items\ndef get_product_url(conf, item):\n    product_link_element = item.find('a', class_='vtex-product-summary-2-x-clearLink')\n    return conf[\"url\"] + product_link_element['href'] if product_link_element else None\ndef get_title(conf, item):\n    title_element = item.find('span', class_='vtex-product-summary-2-x-brandName')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, item):",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "peekOfCode": "def get_product_url(conf, item):\n    product_link_element = item.find('a', class_='vtex-product-summary-2-x-clearLink')\n    return conf[\"url\"] + product_link_element['href'] if product_link_element else None\ndef get_title(conf, item):\n    title_element = item.find('span', class_='vtex-product-summary-2-x-brandName')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, item):\n    price_element = item.find(\"span\", class_=\"vtex-product-price-1-x-sellingPriceValue\")\n    if price_element:\n        currency_code = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyCode\").get_text(strip=True)",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "peekOfCode": "def get_title(conf, item):\n    title_element = item.find('span', class_='vtex-product-summary-2-x-brandName')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, item):\n    price_element = item.find(\"span\", class_=\"vtex-product-price-1-x-sellingPriceValue\")\n    if price_element:\n        currency_code = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyCode\").get_text(strip=True)\n        currency_integer = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyInteger\").get_text(strip=True)\n        currency_decimal = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyDecimal\").get_text(strip=True)\n        currency_fraction = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyFraction\").get_text(strip=True)",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "peekOfCode": "def get_price(conf, item):\n    price_element = item.find(\"span\", class_=\"vtex-product-price-1-x-sellingPriceValue\")\n    if price_element:\n        currency_code = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyCode\").get_text(strip=True)\n        currency_integer = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyInteger\").get_text(strip=True)\n        currency_decimal = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyDecimal\").get_text(strip=True)\n        currency_fraction = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyFraction\").get_text(strip=True)\n        price = f\"{currency_code} {currency_integer}{currency_decimal}{currency_fraction}\"\n        return price\n    return None",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "peekOfCode": "def get_image_url(conf, item):\n    image_element = item.find('img', class_='vtex-product-summary-2-x-imageNormal')\n    return image_element['src'] if image_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.darkness.page_url",
        "description": "src.jobs.slave_page.pages.brazil.darkness.page_url",
        "peekOfCode": "def get_url(conf, url):\n    return url",
        "detail": "src.jobs.slave_page.pages.brazil.darkness.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "peekOfCode": "JOB_NAME = \"dux_nutrition_lab\"\nBRAND = \"dux nutrition lab\"\nURL = \"https://www.duxnutrition.com\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#ProductDescriptionAccordion > div.vtex-flex-layout-0-x-flexRow.vtex-flex-layout-0-x-flexRow--productDescriptionAccordion.vtex-flex-layout-0-x-flexRow--container.vtex-flex-layout-0-x-flexRow--block > section > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "peekOfCode": "BRAND = \"dux nutrition lab\"\nURL = \"https://www.duxnutrition.com\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#ProductDescriptionAccordion > div.vtex-flex-layout-0-x-flexRow.vtex-flex-layout-0-x-flexRow--productDescriptionAccordion.vtex-flex-layout-0-x-flexRow--container.vtex-flex-layout-0-x-flexRow--block > section > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "peekOfCode": "URL = \"https://www.duxnutrition.com\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#ProductDescriptionAccordion > div.vtex-flex-layout-0-x-flexRow.vtex-flex-layout-0-x-flexRow--productDescriptionAccordion.vtex-flex-layout-0-x-flexRow--container.vtex-flex-layout-0-x-flexRow--block > section > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#ProductDescriptionAccordion > div.vtex-flex-layout-0-x-flexRow.vtex-flex-layout-0-x-flexRow--productDescriptionAccordion.vtex-flex-layout-0-x-flexRow--container.vtex-flex-layout-0-x-flexRow--block > section > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#ProductDescriptionAccordion > div.vtex-flex-layout-0-x-flexRow.vtex-flex-layout-0-x-flexRow--productDescriptionAccordion.vtex-flex-layout-0-x-flexRow--container.vtex-flex-layout-0-x-flexRow--block > section > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.dry",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.dry",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    # Busca todos os itens no HTML com a classe especificada\n    items = soup.find_all('div', class_='duxnutrition-search-result-3-x-galleryItem')\n    return items\ndef get_product_url(conf, soup):\n    # Busca o link do produto no HTML\n    product_link_element = soup.find('a', class_='vtex-product-summary-2-x-clearLink')\n    product_link = conf[\"url\"] + product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    # Busca o link do produto no HTML\n    product_link_element = soup.find('a', class_='vtex-product-summary-2-x-clearLink')\n    product_link = conf[\"url\"] + product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    # Busca o título do produto no HTML\n    title_element = soup.find('span', class_='vtex-product-summary-2-x-productBrand')\n    title = title_element.get_text().strip() if title_element else None\n    return title",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    # Busca o título do produto no HTML\n    title_element = soup.find('span', class_='vtex-product-summary-2-x-productBrand')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    # Busca o preço do produto no HTML\n    price = None\n    price_element = soup.find('span', class_='vtex-store-components-3-x-sellingPriceValue')\n    if price_element:",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    # Busca o preço do produto no HTML\n    price = None\n    price_element = soup.find('span', class_='vtex-store-components-3-x-sellingPriceValue')\n    if price_element:\n        price = price_element.get_text().strip()\n    return price\ndef get_image_url(conf, soup):\n    # Busca a URL da imagem do produto no HTML\n    image_element = soup.find('img', class_='vtex-product-summary-2-x-imageNormal')",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    # Busca a URL da imagem do produto no HTML\n    image_element = soup.find('img', class_='vtex-product-summary-2-x-imageNormal')\n    if image_element and 'src' in image_element.attrs:\n        return image_element['src']\n    return None",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_url",
        "description": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(conf[\"index\"])\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.dux_nutrition_lab.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "peekOfCode": "JOB_NAME = \"growth_supplements\"\nBRAND = \"growth supplements\"\nURL = \"https://www.gsuplementos.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#breadcrumb'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "peekOfCode": "BRAND = \"growth supplements\"\nURL = \"https://www.gsuplementos.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#breadcrumb'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "peekOfCode": "URL = \"https://www.gsuplementos.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#breadcrumb'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#breadcrumb'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#breadcrumb'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.dry",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df['name'] = df['name'].str.replace('- growth supplements', '')\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.dry",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    return soup.find_all('a', class_='cardprod text-decoration-none')\ndef get_product_url(conf, soup):\n    import re\n    if not isinstance(soup, str):\n        html_content = str(soup)\n    else:\n        html_content = soup\n    pattern = r'<a[^>]*class=\"[^\"]*cardprod[^\"]*text-decoration-none[^\"]*\"[^>]*href=\"([^\"]+)\"'\n    match = re.search(pattern, html_content)",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    import re\n    if not isinstance(soup, str):\n        html_content = str(soup)\n    else:\n        html_content = soup\n    pattern = r'<a[^>]*class=\"[^\"]*cardprod[^\"]*text-decoration-none[^\"]*\"[^>]*href=\"([^\"]+)\"'\n    match = re.search(pattern, html_content)\n    if match:\n        product_link_element = conf['url'] + match.group(1).strip() ",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('h3', class_='cardprod-nomeProduto-t1')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    price_element = soup.find('span', class_='cardprod-valor')\n    if price_element:\n        # Obtém apenas o texto direto dentro do span, ignorando elementos filhos\n        price_text = ''.join(price_element.find_all(string=True, recursive=False)).strip()\n        # Remove o símbolo \"R$\" e quaisquer espaços\n        price_without_symbol = price_text.strip()",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    price_element = soup.find('span', class_='cardprod-valor')\n    if price_element:\n        # Obtém apenas o texto direto dentro do span, ignorando elementos filhos\n        price_text = ''.join(price_element.find_all(string=True, recursive=False)).strip()\n        # Remove o símbolo \"R$\" e quaisquer espaços\n        price_without_symbol = price_text.strip()\n        return price_without_symbol\n    return None\ndef get_image_url(conf, soup):",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    image_element = soup.find('img')\n    return image_element['src'] if image_element and image_element.has_attr('src') else None",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.growth_supplements.page_url",
        "description": "src.jobs.slave_page.pages.brazil.growth_supplements.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(conf[\"index\"])\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.growth_supplements.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "description": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "peekOfCode": "JOB_NAME = \"ifood\"\nBRAND = \"ifood\"\nURL = \"https://ifood.com.br/\"\nSTATUS = False\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': ''},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "description": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "peekOfCode": "BRAND = \"ifood\"\nURL = \"https://ifood.com.br/\"\nSTATUS = False\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': ''},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "description": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "peekOfCode": "URL = \"https://ifood.com.br/\"\nSTATUS = False\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': ''},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "description": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "peekOfCode": "STATUS = False\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': ''},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "description": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': ''},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "description": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.dry",
        "description": "src.jobs.slave_page.pages.brazil.ifood.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    def_string = \"barrinha \"\n    df['title'] = def_string + df['title']\n    df = transform(CONF, df)\n    df['name'] = df['name'].str.slice(len(def_string))\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.dry",
        "description": "src.jobs.slave_page.pages.brazil.ifood.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('li', class_='product collection-product-list')\n    return items\ndef get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='card-link')\n    product_link = conf[\"url\"] + product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('a', class_='card-title center link-underline card-title-ellipsis')\n    title = title_element.get_text().strip() if title_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='card-link')\n    product_link = conf[\"url\"] + product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('a', class_='card-title center link-underline card-title-ellipsis')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    def format_price(price_text):",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('a', class_='card-title center link-underline card-title-ellipsis')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    def format_price(price_text):\n        cleaned_price = price_text.strip()\n        if \",\" in cleaned_price:\n            return None\n        else:",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    def format_price(price_text):\n        cleaned_price = price_text.strip()\n        if \",\" in cleaned_price:\n            return None\n        else:\n            return cleaned_price + \",00\"\n    price_sale_span = soup.find('span', class_='price-item--sale')\n    if price_sale_span:\n        price_text = price_sale_span.get_text()",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    div_container = soup.find('div', class_='card-product__wrapper')\n    if div_container:\n        image_element = div_container.find('img')\n        if image_element and 'data-srcset' in image_element.attrs:\n            srcset = image_element['data-srcset']\n            images = srcset.split(\",\")\n            middle_image = images[len(images) // 2].split(\" \")[0].strip()\n            image_url = \"https:\" + middle_image if middle_image.startswith(\"//\") else middle_image\n            return image_url",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.ifood.page_url",
        "description": "src.jobs.slave_page.pages.brazil.ifood.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(conf[\"index\"])\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.ifood.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "peekOfCode": "JOB_NAME = \"integralmedica\"\nBRAND = \"integralmedica\"\nURL = \"https://www.integralmedica.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div > div > div > div:nth-child(3) > div > div > div:nth-child(3) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "peekOfCode": "BRAND = \"integralmedica\"\nURL = \"https://www.integralmedica.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div > div > div > div:nth-child(3) > div > div > div:nth-child(3) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "peekOfCode": "URL = \"https://www.integralmedica.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div > div > div > div:nth-child(3) > div > div > div:nth-child(3) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div > div > div > div:nth-child(3) > div > div > div:nth-child(3) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(3) > div > div > div > div > div:nth-child(3) > div > div > div:nth-child(3) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.dry",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.dry",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('div', class_='vtex-search-result-3-x-galleryItem vtex-search-result-3-x-galleryItem--normal vtex-search-result-3-x-galleryItem--grid pa4')\n    return items\ndef get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='vtex-product-summary-2-x-clearLink vtex-product-summary-2-x-clearLink--default h-100 flex flex-column')\n    product_link = conf[\"url\"] + product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('span', class_='vtex-product-summary-2-x-productBrand vtex-product-summary-2-x-brandName t-body')\n    title = title_element.get_text().strip() if title_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='vtex-product-summary-2-x-clearLink vtex-product-summary-2-x-clearLink--default h-100 flex flex-column')\n    product_link = conf[\"url\"] + product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('span', class_='vtex-product-summary-2-x-productBrand vtex-product-summary-2-x-brandName t-body')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    price_element = soup.find('span', class_='vtex-product-price-1-x-sellingPriceValue vtex-product-price-1-x-sellingPriceValue--shelfDefault')",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('span', class_='vtex-product-summary-2-x-productBrand vtex-product-summary-2-x-brandName t-body')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    price_element = soup.find('span', class_='vtex-product-price-1-x-sellingPriceValue vtex-product-price-1-x-sellingPriceValue--shelfDefault')\n    price = price_element.get_text().strip() if price_element else None\n    return price\ndef get_image_url(conf, soup):\n    image_element = soup.find('img', class_='vtex-product-summary-2-x-imageNormal vtex-product-summary-2-x-image vtex-product-summary-2-x-image--shelfDefault')",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    price_element = soup.find('span', class_='vtex-product-price-1-x-sellingPriceValue vtex-product-price-1-x-sellingPriceValue--shelfDefault')\n    price = price_element.get_text().strip() if price_element else None\n    return price\ndef get_image_url(conf, soup):\n    image_element = soup.find('img', class_='vtex-product-summary-2-x-imageNormal vtex-product-summary-2-x-image vtex-product-summary-2-x-image--shelfDefault')\n    link_imagem = image_element['src'] if image_element else None\n    return link_imagem",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    image_element = soup.find('img', class_='vtex-product-summary-2-x-imageNormal vtex-product-summary-2-x-image vtex-product-summary-2-x-image--shelfDefault')\n    link_imagem = image_element['src'] if image_element else None\n    return link_imagem",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.integralmedica.page_url",
        "description": "src.jobs.slave_page.pages.brazil.integralmedica.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(conf[\"index\"])\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.integralmedica.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "peekOfCode": "JOB_NAME = \"iridium_labs\"\nBRAND = \"iridium labs\"\nURL = \"https://www.iridiumlabs.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#shopify-section-template--14719947636839__main > div > div > div > div > div.t4s-col-md-6.t4s-col-12.t4s-col-item.t4s-product__info-wrapper.t4s-pr'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "peekOfCode": "BRAND = \"iridium labs\"\nURL = \"https://www.iridiumlabs.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#shopify-section-template--14719947636839__main > div > div > div > div > div.t4s-col-md-6.t4s-col-12.t4s-col-item.t4s-product__info-wrapper.t4s-pr'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "peekOfCode": "URL = \"https://www.iridiumlabs.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#shopify-section-template--14719947636839__main > div > div > div > div > div.t4s-col-md-6.t4s-col-12.t4s-col-item.t4s-product__info-wrapper.t4s-pr'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#shopify-section-template--14719947636839__main > div > div > div > div > div.t4s-col-md-6.t4s-col-12.t4s-col-item.t4s-product__info-wrapper.t4s-pr'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#shopify-section-template--14719947636839__main > div > div > div > div > div.t4s-col-md-6.t4s-col-12.t4s-col-item.t4s-product__info-wrapper.t4s-pr'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.dry",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.dry",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    import re\n    items = []\n    items_container = soup.find_all('div', class_='t4s-product-wrapper')\n    for item in items_container:\n        if (not re.search(\"cdn.shopify.com\", str(item))):\n            items.append(item)\n    return items\ndef get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='t4s-full-width-link')",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='t4s-full-width-link')\n    return conf[\"url\"] + product_link_element['href'] if product_link_element else None\ndef get_title(conf, soup):\n    title_element = soup.find('h3', class_='t4s-product-title')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    import re\n    final_price = None\n    price_container = soup.find(class_=\"t4s-product-price\")",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('h3', class_='t4s-product-title')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    import re\n    final_price = None\n    price_container = soup.find(class_=\"t4s-product-price\")\n    if price_container:\n        # Verifica se existe um desconto\n        discount_price_element = price_container.find(\"ins\")",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    import re\n    final_price = None\n    price_container = soup.find(class_=\"t4s-product-price\")\n    if price_container:\n        # Verifica se existe um desconto\n        discount_price_element = price_container.find(\"ins\")\n        if discount_price_element:\n            final_price = discount_price_element.text.strip()\n        else:",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    image_element = soup.find(class_='t4s-product-main-img')\n    if (image_element and 'srcset' in image_element.attrs):\n        srcset = image_element['srcset']\n        image_urls = [img.strip() for img in srcset.split(',')]\n        highest_res_image = image_urls[-1].split(' ')[0]\n        image_link = \"https:\" + highest_res_image\n        return image_link\n    elif (image_element and 'src' in image_element.attrs):\n        image_link = \"https:\" + image_element['src']",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.iridium_labs.page_url",
        "description": "src.jobs.slave_page.pages.brazil.iridium_labs.page_url",
        "peekOfCode": "def get_url(conf, url):\n    return url",
        "detail": "src.jobs.slave_page.pages.brazil.iridium_labs.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "peekOfCode": "JOB_NAME = \"max_titanium\"\nBRAND = \"max titanium\"\nURL = \"https://www.maxtitanium.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(5) > div > div:nth-child(2) > div:nth-child(6) > section > div > div > div > div:nth-child(1) > div > div > div > div > div:nth-child(2) > div > div > div > div > table > tbody'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 500,",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "peekOfCode": "BRAND = \"max titanium\"\nURL = \"https://www.maxtitanium.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(5) > div > div:nth-child(2) > div:nth-child(6) > section > div > div > div > div:nth-child(1) > div > div > div > div > div:nth-child(2) > div > div > div > div > table > tbody'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 500,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "peekOfCode": "URL = \"https://www.maxtitanium.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(5) > div > div:nth-child(2) > div:nth-child(6) > section > div > div > div > div:nth-child(1) > div > div > div > div > div:nth-child(2) > div > div > div > div > table > tbody'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(5) > div > div:nth-child(2) > div:nth-child(6) > section > div > div > div > div:nth-child(1) > div > div > div > div > div:nth-child(2) > div > div > div > div > table > tbody'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(5) > div > div:nth-child(2) > div:nth-child(6) > section > div > div > div > div:nth-child(1) > div > div > div > div > div:nth-child(2) > div > div > div > div > table > tbody'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.dry",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.dry",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('div', class_='vtex-search-result-3-x-galleryItem')\n    return items\ndef get_product_url(conf, item):\n    product_link_element = item.find('a', class_='vtex-product-summary-2-x-clearLink')\n    return conf[\"url\"] + product_link_element['href'] if product_link_element else None\ndef get_title(conf, item):\n    title_element = item.find('span', class_='vtex-product-summary-2-x-brandName')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, item):",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "peekOfCode": "def get_product_url(conf, item):\n    product_link_element = item.find('a', class_='vtex-product-summary-2-x-clearLink')\n    return conf[\"url\"] + product_link_element['href'] if product_link_element else None\ndef get_title(conf, item):\n    title_element = item.find('span', class_='vtex-product-summary-2-x-brandName')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, item):\n    price_element = item.find(\"span\", class_=\"vtex-product-price-1-x-sellingPriceWithTax\")\n    if price_element:\n        currency_code = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyCode\").get_text(strip=True)",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "peekOfCode": "def get_title(conf, item):\n    title_element = item.find('span', class_='vtex-product-summary-2-x-brandName')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, item):\n    price_element = item.find(\"span\", class_=\"vtex-product-price-1-x-sellingPriceWithTax\")\n    if price_element:\n        currency_code = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyCode\").get_text(strip=True)\n        currency_integer = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyInteger\").get_text(strip=True)\n        currency_decimal = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyDecimal\").get_text(strip=True)\n        currency_fraction = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyFraction\").get_text(strip=True)",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "peekOfCode": "def get_price(conf, item):\n    price_element = item.find(\"span\", class_=\"vtex-product-price-1-x-sellingPriceWithTax\")\n    if price_element:\n        currency_code = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyCode\").get_text(strip=True)\n        currency_integer = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyInteger\").get_text(strip=True)\n        currency_decimal = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyDecimal\").get_text(strip=True)\n        currency_fraction = price_element.find(\"span\", class_=\"vtex-product-price-1-x-currencyFraction\").get_text(strip=True)\n        price = f\"{currency_code} {currency_integer}{currency_decimal}{currency_fraction}\"\n        return price\n    return None",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "peekOfCode": "def get_image_url(conf, item):\n    image_element = item.find('img', class_='vtex-product-summary-2-x-imageNormal')\n    return image_element['src'] if image_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.max_titanium.page_url",
        "description": "src.jobs.slave_page.pages.brazil.max_titanium.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(conf[\"index\"])\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.max_titanium.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "peekOfCode": "JOB_NAME = \"new_millen\"\nBRAND = \"new millen\"\nURL = \"https://loja.newmillen.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#product-description'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "peekOfCode": "BRAND = \"new millen\"\nURL = \"https://loja.newmillen.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#product-description'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "peekOfCode": "URL = \"https://loja.newmillen.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#product-description'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#product-description'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#product-description'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.dry",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.dry",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('div', class_='item item-rounded item-product box-rounded p-0')\n    return items\ndef get_product_url(conf, soup):\n    product_link_container = soup.find('div', class_='position-relative')\n    if (product_link_container):\n        product_link_element = product_link_container.find('a')\n        return product_link_element['href'] if product_link_element else None\ndef get_title(conf, soup):\n    title_element = soup.find('div', class_='js-item-name item-name mb-3')",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_container = soup.find('div', class_='position-relative')\n    if (product_link_container):\n        product_link_element = product_link_container.find('a')\n        return product_link_element['href'] if product_link_element else None\ndef get_title(conf, soup):\n    title_element = soup.find('div', class_='js-item-name item-name mb-3')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    price_element = soup.find(\"span\", class_=\"js-price-display item-price\")",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('div', class_='js-item-name item-name mb-3')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    price_element = soup.find(\"span\", class_=\"js-price-display item-price\")\n    price = price_element.get_text() if price_element else None\n    return price\ndef get_image_url(conf, soup):\n    image_container = soup.find('div', class_='position-relative')\n    if (image_container):",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    price_element = soup.find(\"span\", class_=\"js-price-display item-price\")\n    price = price_element.get_text() if price_element else None\n    return price\ndef get_image_url(conf, soup):\n    image_container = soup.find('div', class_='position-relative')\n    if (image_container):\n        image_element = soup.find('img')\n        return \"https:\" + image_element['srcset'].split(\" \")[0] if image_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    image_container = soup.find('div', class_='position-relative')\n    if (image_container):\n        image_element = soup.find('img')\n        return \"https:\" + image_element['srcset'].split(\" \")[0] if image_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.new_millen.page_url",
        "description": "src.jobs.slave_page.pages.brazil.new_millen.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(conf[\"index\"])\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.new_millen.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "peekOfCode": "JOB_NAME = \"nutrata\"\nBRAND = \"nutrata\"\nURL = \"https://loja.nutrata.com.br/\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#descricao > div.board'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "peekOfCode": "BRAND = \"nutrata\"\nURL = \"https://loja.nutrata.com.br/\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#descricao > div.board'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "peekOfCode": "URL = \"https://loja.nutrata.com.br/\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#descricao > div.board'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#descricao > div.board'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#descricao > div.board'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.dry",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.dry",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items_container = soup.find_all('div', class_='product-card')\n    items = [item for item in items_container if item.find('div', class_='product-price-sale') is not None]\n    return items\ndef get_product_url(conf, soup):\n    product_link_element = soup.find('a', href=True)\n    product_link = product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('p', class_='product-title')",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_element = soup.find('a', href=True)\n    product_link = product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('p', class_='product-title')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    price_element = soup.find('div', class_='product-price-sale')",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('p', class_='product-title')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    price_element = soup.find('div', class_='product-price-sale')\n    price = price_element.get_text().strip() if price_element else None\n    return price\ndef get_image_url(conf, soup):\n    import re",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    price_element = soup.find('div', class_='product-price-sale')\n    price = price_element.get_text().strip() if price_element else None\n    return price\ndef get_image_url(conf, soup):\n    import re\n    image_container = soup.find('div', class_='product-image')\n    link_imagem = None\n    if image_container:\n        style_attr = image_container.get('style', '')",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    import re\n    image_container = soup.find('div', class_='product-image')\n    link_imagem = None\n    if image_container:\n        style_attr = image_container.get('style', '')\n        match = re.search(r'url\\(\"([^\"]+)\"\\)', style_attr)\n        link_imagem = match.group(1) if match else None\n    return link_imagem",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.nutrata.page_url",
        "description": "src.jobs.slave_page.pages.brazil.nutrata.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(1)\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.nutrata.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "peekOfCode": "JOB_NAME = \"probiotica\"\nBRAND = \"probiotica\"\nURL = \"https://www.probiotica.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(4) > div > div:nth-child(2) > div:nth-child(2) > section > div > div > div > div:nth-child(1) > div > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "peekOfCode": "BRAND = \"probiotica\"\nURL = \"https://www.probiotica.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(4) > div > div:nth-child(2) > div:nth-child(2) > section > div > div > div > div:nth-child(1) > div > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "peekOfCode": "URL = \"https://www.probiotica.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(4) > div > div:nth-child(2) > div:nth-child(2) > section > div > div > div > div:nth-child(1) > div > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(4) > div > div:nth-child(2) > div:nth-child(2) > section > div > div > div > div:nth-child(1) > div > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(4) > div > div:nth-child(2) > div:nth-child(2) > section > div > div > div > div:nth-child(1) > div > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.dry",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.dry",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('div', class_='vtex-search-result-3-x-galleryItem')\n    return items\ndef get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='vtex-product-summary-2-x-clearLink')\n    product_link = conf[\"url\"] + product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('h2', class_='vtex-product-summary-2-x-productNameContainer')\n    if title_element:",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='vtex-product-summary-2-x-clearLink')\n    product_link = conf[\"url\"] + product_link_element['href'] if product_link_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('h2', class_='vtex-product-summary-2-x-productNameContainer')\n    if title_element:\n        span_element = title_element.find('span', class_='vtex-product-summary-2-x-productBrand')\n        title = span_element.text.strip() if span_element else None\n    else:",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('h2', class_='vtex-product-summary-2-x-productNameContainer')\n    if title_element:\n        span_element = title_element.find('span', class_='vtex-product-summary-2-x-productBrand')\n        title = span_element.text.strip() if span_element else None\n    else:\n        title = None\n    return title\ndef get_price(conf, soup):\n    price = None",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    price = None\n    price_element = soup.find('span', class_='vtex-product-price-1-x-sellingPriceValue')\n    if price_element:\n        currency_code = price_element.find('span', class_='vtex-product-price-1-x-currencyCode').text.strip()\n        currency_literal = price_element.find('span', class_='vtex-product-price-1-x-currencyLiteral').text.strip()\n        currency_integer = price_element.find('span', class_='vtex-product-price-1-x-currencyInteger').text.strip()\n        currency_decimal = price_element.find('span', class_='vtex-product-price-1-x-currencyDecimal').text.strip()\n        currency_fraction = price_element.find('span', class_='vtex-product-price-1-x-currencyFraction').text.strip()\n        price = f\"{currency_code}{currency_literal}{currency_integer}{currency_decimal}{currency_fraction}\"",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    image_element = soup.find('img', class_='vtex-product-summary-2-x-imageNormal vtex-product-summary-2-x-image')\n    link_imagem = image_element['src'] if image_element else None\n    return link_imagem",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.probiotica.page_url",
        "description": "src.jobs.slave_page.pages.brazil.probiotica.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(conf[\"index\"])\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.probiotica.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "description": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "peekOfCode": "JOB_NAME = \"puravida\"\nBRAND = \"puravida\"\nURL = \"https://www.puravida.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#product-content > div:nth-child(2) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "description": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "peekOfCode": "BRAND = \"puravida\"\nURL = \"https://www.puravida.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#product-content > div:nth-child(2) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "description": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "peekOfCode": "URL = \"https://www.puravida.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#product-content > div:nth-child(2) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "description": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#product-content > div:nth-child(2) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "description": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#product-content > div:nth-child(2) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "description": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.dry",
        "description": "src.jobs.slave_page.pages.brazil.puravida.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.dry",
        "description": "src.jobs.slave_page.pages.brazil.puravida.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    # Procura por todos os itens de produtos na página\n    items = soup.select('div[id^=\"product-new-spot-item-\"]')\n    return items\ndef get_product_url(conf, soup):\n    import re\n    # Converte o objeto soup para string, se necessário\n    if not isinstance(soup, str):\n        html_content = str(soup)\n    else:",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    import re\n    # Converte o objeto soup para string, se necessário\n    if not isinstance(soup, str):\n        html_content = str(soup)\n    else:\n        html_content = soup\n    # Expressão regular para encontrar o href dentro da tag <a> com a classe 'spot-product-info'\n    pattern = r'<a[^>]*class=\"[^\"]*spot-product-info[^\"]*\"[^>]*href=\"([^\"]+)\"'\n    # Buscar a primeira correspondência do padrão",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    # Procura pelo elemento 'p' com a classe 'product-name' para obter o título\n    title_element = soup.find('p', class_='product-name')\n    # Extrai o texto e remove espaços em branco desnecessários, se o elemento for encontrado\n    title = title_element.text.strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    import re\n    # Procura pela div que contém o preço\n    price_element = soup.find('div', class_='product-price')",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    import re\n    # Procura pela div que contém o preço\n    price_element = soup.find('div', class_='product-price')\n    if price_element:\n        price_text = price_element.get_text(strip=True)\n        # Usa regex para capturar o valor numérico após 'R$'\n        match = re.search(r'R\\$\\s*([\\d.,]+)', price_text)\n        if match:\n            price = match.group(1)",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    # Procura pelo primeiro elemento 'img' dentro de um 'picture' associado ao produto\n    image_container = soup.find('picture')\n    image_element = image_container.find('img') if image_container else None\n    # Extrai o atributo 'src' do elemento de imagem, se encontrado\n    link_imagem = image_element['src'] if image_element else None\n    return link_imagem",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.puravida.page_url",
        "description": "src.jobs.slave_page.pages.brazil.puravida.page_url",
        "peekOfCode": "def get_url(conf, url):\n    return url",
        "detail": "src.jobs.slave_page.pages.brazil.puravida.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "description": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "peekOfCode": "JOB_NAME = \"truesource\"\nBRAND = \"truesource\"\nURL = \"https://www.truesource.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#\\\\39 21277609-0 > div > div:nth-child(2)'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "description": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "peekOfCode": "BRAND = \"truesource\"\nURL = \"https://www.truesource.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#\\\\39 21277609-0 > div > div:nth-child(2)'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "description": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "peekOfCode": "URL = \"https://www.truesource.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#\\\\39 21277609-0 > div > div:nth-child(2)'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "description": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#\\\\39 21277609-0 > div > div:nth-child(2)'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "description": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': '#\\\\39 21277609-0 > div > div:nth-child(2)'}\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "description": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 1,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.dry",
        "description": "src.jobs.slave_page.pages.brazil.truesource.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df['name'] = df['name'].str.replace(' - True Source', '')\n    df['name'] = df['name'].str.replace('True Source', '')\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.dry",
        "description": "src.jobs.slave_page.pages.brazil.truesource.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('div', class_='w-full lg:max-w-[260px]')\n    return items\ndef get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='flex items-center justify-center relative h-full bg-ice')\n    product_url = conf[\"url\"] + product_link_element['href'] if product_link_element else None\n    return product_url\ndef get_title(conf, soup):\n    title_element = soup.find('h3', class_='text-dark text-sm text-ellipsis font-bold line-clamp-2 h-10')\n    title = title_element.get_text(strip=True) if title_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_element = soup.find('a', class_='flex items-center justify-center relative h-full bg-ice')\n    product_url = conf[\"url\"] + product_link_element['href'] if product_link_element else None\n    return product_url\ndef get_title(conf, soup):\n    title_element = soup.find('h3', class_='text-dark text-sm text-ellipsis font-bold line-clamp-2 h-10')\n    title = title_element.get_text(strip=True) if title_element else None\n    return title\ndef get_price(conf, soup):\n    price_element = soup.find('span', class_='text-dark text-xs lg:text-sm')",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('h3', class_='text-dark text-sm text-ellipsis font-bold line-clamp-2 h-10')\n    title = title_element.get_text(strip=True) if title_element else None\n    return title\ndef get_price(conf, soup):\n    price_element = soup.find('span', class_='text-dark text-xs lg:text-sm')\n    price = price_element.get_text(strip=True) if price_element else None\n    return price\ndef get_image_url(conf, soup):\n    image_element = soup.find('img')",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    price_element = soup.find('span', class_='text-dark text-xs lg:text-sm')\n    price = price_element.get_text(strip=True) if price_element else None\n    return price\ndef get_image_url(conf, soup):\n    image_element = soup.find('img')\n    image_url = image_element['src'] if image_element else None\n    return image_url",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    image_element = soup.find('img')\n    image_url = image_element['src'] if image_element else None\n    return image_url",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.truesource.page_url",
        "description": "src.jobs.slave_page.pages.brazil.truesource.page_url",
        "peekOfCode": "def get_url(conf, url):\n    if (not conf[\"index\"]):\n        conf[\"index\"] = 1\n        return url + str(conf[\"index\"])\n    conf[\"index\"] += 1\n    return url + str(conf[\"index\"])",
        "detail": "src.jobs.slave_page.pages.brazil.truesource.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "peekOfCode": "JOB_NAME = \"under_labz\"\nBRAND = \"under labz\"\nURL = \"https://underlabz.com.br\"\nSTATUS = False\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': ''},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "peekOfCode": "BRAND = \"under labz\"\nURL = \"https://underlabz.com.br\"\nSTATUS = False\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': ''},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "peekOfCode": "URL = \"https://underlabz.com.br\"\nSTATUS = False\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': ''},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "peekOfCode": "STATUS = False\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': ''},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': ''},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.5,\n    \"scroll_step\": 1000,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.3,\n    \"max_return\": 4000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.dry",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.dry",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('li', class_='collection-product-card')\n    return items\ndef get_product_url(conf, soup):\n    product_link_element = soup.find('a')\n    return conf[\"url\"] + product_link_element['href'] if product_link_element else None\ndef get_title(conf, soup):\n    title_element = soup.find('h3', class_='card__title')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_link_element = soup.find('a')\n    return conf[\"url\"] + product_link_element['href'] if product_link_element else None\ndef get_title(conf, soup):\n    title_element = soup.find('h3', class_='card__title')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    # Busca o elemento de preço na seção de preços em promoção\n    price_element = soup.find('span', class_='price-item price-item--sale')\n    # Caso não haja preço em promoção, busca o preço regular",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('h3', class_='card__title')\n    return title_element.get_text().strip() if title_element else None\ndef get_price(conf, soup):\n    # Busca o elemento de preço na seção de preços em promoção\n    price_element = soup.find('span', class_='price-item price-item--sale')\n    # Caso não haja preço em promoção, busca o preço regular\n    if not price_element:\n        price_element = soup.find('span', class_='price-item price-item--regular')\n    return price_element.get_text().strip() if price_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    # Busca o elemento de preço na seção de preços em promoção\n    price_element = soup.find('span', class_='price-item price-item--sale')\n    # Caso não haja preço em promoção, busca o preço regular\n    if not price_element:\n        price_element = soup.find('span', class_='price-item price-item--regular')\n    return price_element.get_text().strip() if price_element else None\ndef get_image_url(conf, soup):\n    image_element = soup.find('img', class_='motion-reduce media--first')\n    return \"https:\" + image_element['src'] if image_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    image_element = soup.find('img', class_='motion-reduce media--first')\n    return \"https:\" + image_element['src'] if image_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.under_labz.page_url",
        "description": "src.jobs.slave_page.pages.brazil.under_labz.page_url",
        "peekOfCode": "def get_url(conf, url):\n    return url",
        "detail": "src.jobs.slave_page.pages.brazil.under_labz.page_url",
        "documentation": {}
    },
    {
        "label": "JOB_NAME",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "peekOfCode": "JOB_NAME = \"vitafor\"\nBRAND = \"vitafor\"\nURL = \"https://www.vitafor.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(5) > div > div:nth-child(4) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.7,\n    \"scroll_step\": 1500,",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "documentation": {}
    },
    {
        "label": "BRAND",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "peekOfCode": "BRAND = \"vitafor\"\nURL = \"https://www.vitafor.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(5) > div > div:nth-child(4) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.7,\n    \"scroll_step\": 1500,\n    \"percentage\": 0.07,",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "peekOfCode": "URL = \"https://www.vitafor.com.br\"\nSTATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(5) > div > div:nth-child(4) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.7,\n    \"scroll_step\": 1500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.1,",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "documentation": {}
    },
    {
        "label": "STATUS",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "peekOfCode": "STATUS = True\nPRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(5) > div > div:nth-child(4) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.7,\n    \"scroll_step\": 1500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.1,\n    \"max_return\": 2000,",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "documentation": {}
    },
    {
        "label": "PRODUCT_DEFINITION_TAG_MAP",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "peekOfCode": "PRODUCT_DEFINITION_TAG_MAP = [\n    {'tag': None, 'path': 'body > div.render-container.render-route-store-product > div > div.vtex-store__template.bg-base > div > div > div > div:nth-child(5) > div > div:nth-child(4) > div'},\n]\nDYNAMIC_SCROLL = {\n    \"time_sleep\": 0.7,\n    \"scroll_step\": 1500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.1,\n    \"max_return\": 2000,\n    \"max_attempts\": 3",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "documentation": {}
    },
    {
        "label": "DYNAMIC_SCROLL",
        "kind": 5,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "peekOfCode": "DYNAMIC_SCROLL = {\n    \"time_sleep\": 0.7,\n    \"scroll_step\": 1500,\n    \"percentage\": 0.07,\n    \"return_percentage\": 0.1,\n    \"max_return\": 2000,\n    \"max_attempts\": 3\n}",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.conf",
        "documentation": {}
    },
    {
        "label": "create_origin_dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.dry",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.dry",
        "peekOfCode": "def create_origin_dry():\n    file_path = CONF['data_path']\n    df = pd.read_csv(file_path + \"/products_extract_csl.csv\")\n    df = transform(CONF, df)\n    df['name'] = df['name'].str.replace(' - vitafor', '')\n    df.to_csv(file_path + \"/products_transform_csl.csv\", index=False)\n    print(\"Success in saving products_transform_csl\")\ndef dry(conf):\n    global CONF\n    CONF = conf",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.dry",
        "documentation": {}
    },
    {
        "label": "dry",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.dry",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.dry",
        "peekOfCode": "def dry(conf):\n    global CONF\n    CONF = conf\n    print(\"Data Dry\")\n    create_origin_dry()",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.dry",
        "documentation": {}
    },
    {
        "label": "get_items",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "peekOfCode": "def get_items(conf, soup):\n    items = soup.find_all('section', class_='vtex-product-summary-2-x-container')\n    return items\ndef get_product_url(conf, soup):\n    product_element = soup.find(class_='vtex-product-summary-2-x-clearLink')\n    product_link = conf[\"url\"] + product_element['href'] if product_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('h3', class_='vtex-product-summary-2-x-productNameContainer')\n    title = title_element.get_text().strip() if title_element else None",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "documentation": {}
    },
    {
        "label": "get_product_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "peekOfCode": "def get_product_url(conf, soup):\n    product_element = soup.find(class_='vtex-product-summary-2-x-clearLink')\n    product_link = conf[\"url\"] + product_element['href'] if product_element else None\n    return product_link\ndef get_title(conf, soup):\n    title_element = soup.find('h3', class_='vtex-product-summary-2-x-productNameContainer')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    # Encontra o contêiner que contém o preço",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "documentation": {}
    },
    {
        "label": "get_title",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "peekOfCode": "def get_title(conf, soup):\n    title_element = soup.find('h3', class_='vtex-product-summary-2-x-productNameContainer')\n    title = title_element.get_text().strip() if title_element else None\n    return title\ndef get_price(conf, soup):\n    # Encontra o contêiner que contém o preço\n    price_container = soup.find('span', class_='vitafor-store-theme-9-x-customPrice')\n    if price_container:\n        # Captura o texto do preço e remove espaços extras\n        price = price_container.get_text(strip=True)",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "documentation": {}
    },
    {
        "label": "get_price",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "peekOfCode": "def get_price(conf, soup):\n    # Encontra o contêiner que contém o preço\n    price_container = soup.find('span', class_='vitafor-store-theme-9-x-customPrice')\n    if price_container:\n        # Captura o texto do preço e remove espaços extras\n        price = price_container.get_text(strip=True)\n        return price\n    else:\n        return None\ndef get_image_url(conf, soup):",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "documentation": {}
    },
    {
        "label": "get_image_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "peekOfCode": "def get_image_url(conf, soup):\n    image_element = soup.find('img', class_='vtex-product-summary-2-x-imageNormal')\n    image_link = image_element['src'] if image_element else None\n    return image_link",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.page_elements",
        "documentation": {}
    },
    {
        "label": "get_url",
        "kind": 2,
        "importPath": "src.jobs.slave_page.pages.brazil.vitafor.page_url",
        "description": "src.jobs.slave_page.pages.brazil.vitafor.page_url",
        "peekOfCode": "def get_url(conf, url):\n    return url",
        "detail": "src.jobs.slave_page.pages.brazil.vitafor.page_url",
        "documentation": {}
    },
    {
        "label": "crawler",
        "kind": 2,
        "importPath": "src.lib.extract.crawler",
        "description": "src.lib.extract.crawler",
        "peekOfCode": "def crawler(page, url):\n    message(\"exec crawler\")\n    if ((\"driver\" not in page.conf.keys()) or (not page.conf[\"driver\"])):\n        message(\"initialize_selenium\")\n        page.conf[\"driver\"] = se.initialize_selenium()\n    load_page(page, url)\ndef load_page(page, url):\n    message(\"exec load_page\")\n    driver = page.conf[\"driver\"]\n    if (page.conf['products_update']):",
        "detail": "src.lib.extract.crawler",
        "documentation": {}
    },
    {
        "label": "load_page",
        "kind": 2,
        "importPath": "src.lib.extract.crawler",
        "description": "src.lib.extract.crawler",
        "peekOfCode": "def load_page(page, url):\n    message(\"exec load_page\")\n    driver = page.conf[\"driver\"]\n    if (page.conf['products_update']):\n        message(\"PRODUCTS_UPDATE\")\n        message(\"3 seconds\")\n        time.sleep(3)\n        element_selector = None\n        se.load_url(driver, url, element_selector)\n        time_sleep = page.conf['dynamic_scroll']['time_sleep']",
        "detail": "src.lib.extract.crawler",
        "documentation": {}
    },
    {
        "label": "extract_data",
        "kind": 2,
        "importPath": "src.lib.extract.crawler",
        "description": "src.lib.extract.crawler",
        "peekOfCode": "def extract_data(page, soup):\n    message(\"exec extract_data\")\n    path_products_extract_temp = page.conf['path_products_extract_temp']\n    df_products_temp = create_or_read_df(path_products_extract_temp, page.conf['df_products'].columns)\n    size_products_temp = len(df_products_temp)\n    items = page.get_items(soup)\n    size_items = len(items)\n    message(f\"size_items = {size_items}\")\n    page.conf['size_items'] = size_items\n    if (size_items == 0):",
        "detail": "src.lib.extract.crawler",
        "documentation": {}
    },
    {
        "label": "extract",
        "kind": 2,
        "importPath": "src.lib.extract.extract",
        "description": "src.lib.extract.extract",
        "peekOfCode": "def extract(conf: dict):\n    message(\"EXTRACT\")\n    page = Page(conf)\n    if (conf['exec_flag'] == \"new_page\"):\n        message(\"initializing_new_page\")\n        delete_directory_and_contents(f\"{conf[\"data_path\"]}/*\")\n        conf[\"products_update\"] = True\n        products_update(page)\n        conf[\"products_update\"] = False\n        conf[\"products_metadata_update\"] = True",
        "detail": "src.lib.extract.extract",
        "documentation": {}
    },
    {
        "label": "products_update",
        "kind": 2,
        "importPath": "src.lib.extract.extract",
        "description": "src.lib.extract.extract",
        "peekOfCode": "def products_update(page):\n    seed_path = page.conf['seed_path'] + \"/seed.json\"\n    seeds = read_json(seed_path)\n    page.conf['products_extract_csl'] = f\"{page.conf['data_path']}/products_extract_csl.csv\"\n    page.conf['path_products_extract_temp'] = f\"{page.conf['data_path']}/products_extract_temp.csv\"\n    delete_file(page.conf['path_products_extract_temp'])\n    columns = [\"ref\", \"title\" ,\"price\" ,\"image_url\", \"product_url\", \"ing_date\"]\n    df_products = create_or_read_df(page.conf['products_extract_csl'], columns)\n    data_atual = date.today()\n    page.conf['formatted_date'] = data_atual.strftime(DATE_FORMAT)",
        "detail": "src.lib.extract.extract",
        "documentation": {}
    },
    {
        "label": "products_metadata_update",
        "kind": 2,
        "importPath": "src.lib.extract.extract",
        "description": "src.lib.extract.extract",
        "peekOfCode": "def products_metadata_update(page):\n    message(\"products_metadata_update\")\n    page.conf['products_extract_csl'] = f\"{page.conf['data_path']}/products_extract_csl.csv\"\n    df_products_extract_csl = pd.read_csv(page.conf['products_extract_csl'])\n    urls = df_products_extract_csl['product_url'].values\n    for value, url in enumerate(urls):\n        size_urls = len(urls) - 1\n        message(f\"seed: {url}\")\n        message(f\"index: {value} / {size_urls}\")\n        crawler(page, url)",
        "detail": "src.lib.extract.extract",
        "documentation": {}
    },
    {
        "label": "products_metadata_update_old_pages",
        "kind": 2,
        "importPath": "src.lib.extract.extract",
        "description": "src.lib.extract.extract",
        "peekOfCode": "def products_metadata_update_old_pages(page):\n    message(\"PRODUCTS_METADATA_UPDATE_OLD_PAGES\")\n    products_extract_csl = f\"{page.conf['data_path']}/products_extract_csl.csv\"\n    page.conf['products_extract_csl'] = products_extract_csl\n    df_products_extract_csl = pd.read_csv(products_extract_csl)\n    pagas_path = page.conf['data_path'] + \"/products\"\n    old_files = get_old_files_by_percent(pagas_path, True, 5)\n    refs = [i.replace(\".txt\", \"\") for i in old_files]\n    df_products_extract_csl = df_products_extract_csl[df_products_extract_csl['ref'].isin(refs)]\n    refs_to_delete = list(set(refs) - set(df_products_extract_csl['ref']))",
        "detail": "src.lib.extract.extract",
        "documentation": {}
    },
    {
        "label": "products_metadata_update_old_pages_by_ref",
        "kind": 2,
        "importPath": "src.lib.extract.extract",
        "description": "src.lib.extract.extract",
        "peekOfCode": "def products_metadata_update_old_pages_by_ref(conf: dict, Page: object, url: str):\n    message(\"update_old_page by ref if page is with error in tags\")\n    conf[\"scroll_page\"] = True\n    conf[\"status_job\"] = False\n    conf[\"products_metadata_update\"] = True\n    conf[\"products_update\"] = False\n    page = Page(conf)\n    message(f\"seed: {url}\")\n    crawler(page, url)\ndef products_metadata_create_pages_if_not_exist(page):",
        "detail": "src.lib.extract.extract",
        "documentation": {}
    },
    {
        "label": "products_metadata_create_pages_if_not_exist",
        "kind": 2,
        "importPath": "src.lib.extract.extract",
        "description": "src.lib.extract.extract",
        "peekOfCode": "def products_metadata_create_pages_if_not_exist(page):\n    message(\"PRODUCTS_METADATA_CREATE_PAGES_IF_NOT_EXIST\")\n    products_extract_csl = f\"{page.conf['data_path']}/products_extract_csl.csv\"\n    page.conf['products_extract_csl'] = products_extract_csl\n    df_products_extract_csl = pd.read_csv(products_extract_csl)\n    pagas_path = page.conf['data_path'] + \"/products\"\n    all_pages = [i for i in list_directory(pagas_path)]\n    refs = [f\"{i}.txt\" for i in df_products_extract_csl['ref'].values]\n    pages_to_create = [i.replace(\".txt\", \"\") for i in refs if i not in all_pages]\n    df_products_extract_csl = df_products_extract_csl[df_products_extract_csl[\"ref\"].isin(pages_to_create)]",
        "detail": "src.lib.extract.extract",
        "documentation": {}
    },
    {
        "label": "Page",
        "kind": 6,
        "importPath": "src.lib.extract.page_elements",
        "description": "src.lib.extract.page_elements",
        "peekOfCode": "class Page():\n    def __init__(self, conf) -> None:\n        self.conf = conf\n        self.conf[\"index\"] = None\n        page_type = self.conf[\"page_type\"]\n        country = self.conf[\"country\"]\n        page_name = self.conf[\"page_name\"]\n        page_elements_str = f\"src.jobs.slave_page.pages.{country}.{page_name}.page_elements\"\n        self.page_elements = importlib.import_module(page_elements_str)\n        page_url_str = f\"src.jobs.slave_page.pages.{country}.{page_name}.page_url\"",
        "detail": "src.lib.extract.page_elements",
        "documentation": {}
    },
    {
        "label": "initialize_selenium",
        "kind": 2,
        "importPath": "src.lib.extract.selenium_service",
        "description": "src.lib.extract.selenium_service",
        "peekOfCode": "def initialize_selenium():\n    options = webdriver.ChromeOptions()\n    display = os.getenv('DISPLAY')\n    message(f\"DISPLAY - {display}\")\n    options.add_argument(\"--no-sandbox\")\n    options.add_argument(\"--disable-dev-shm-usage\")\n    options.add_argument(\"--disable-software-rasterizer\")\n    options.add_argument(\"--disable-gpu\")\n    options.add_argument(\"--remote-debugging-port=9222\")\n    options.add_argument(\"--window-size=1920,1080\")",
        "detail": "src.lib.extract.selenium_service",
        "documentation": {}
    },
    {
        "label": "load_url",
        "kind": 2,
        "importPath": "src.lib.extract.selenium_service",
        "description": "src.lib.extract.selenium_service",
        "peekOfCode": "def load_url(driver, url, element_selector=None, timeout=30):\n    driver.get(url)\n    driver.implicitly_wait(100)\n    if element_selector:\n        try:\n            element_present = EC.presence_of_element_located((By.CSS_SELECTOR, element_selector))\n            WebDriverWait(driver, timeout).until(element_present)\n        except TimeoutException:\n            message(f\"Timed out waiting for element {element_selector} to be present\")\ndef get_page_source(driver, retry_delay=5):",
        "detail": "src.lib.extract.selenium_service",
        "documentation": {}
    },
    {
        "label": "get_page_source",
        "kind": 2,
        "importPath": "src.lib.extract.selenium_service",
        "description": "src.lib.extract.selenium_service",
        "peekOfCode": "def get_page_source(driver, retry_delay=5):\n    try:\n        message(\"Tentando obter a página atual...\")\n        page_html = driver.page_source\n        soup = BeautifulSoup(page_html, 'html.parser')\n        return soup, page_html  # Retorna o objeto BeautifulSoup se o page_source for obtido com sucesso\n    except WebDriverException as e:\n        message(f\"Erro ao obter o page_source: {e}. Tentando recarregar após {retry_delay} segundos...\")\n        time.sleep(retry_delay)  # Espera antes de tentar novamente\n        try:",
        "detail": "src.lib.extract.selenium_service",
        "documentation": {}
    },
    {
        "label": "dynamic_scroll",
        "kind": 2,
        "importPath": "src.lib.extract.selenium_service",
        "description": "src.lib.extract.selenium_service",
        "peekOfCode": "def dynamic_scroll(driver, time_sleep=0.7, scroll_step=1000, percentage=0.06, return_percentage=0.3, max_return=4000, max_attempts=3):\n    total_height = driver.execute_script(\"return document.body.scrollHeight\")\n    scroll_increment = min(total_height * percentage, scroll_step)\n    last_scrolled_height = 0\n    attempt_count = 0  # Contador para rastrear tentativas sem mudança na posição de rolagem\n    while True:\n        driver.execute_script(f\"window.scrollBy(0, {scroll_increment});\")\n        time.sleep(time_sleep)  # Espera para o carregamento do conteúdo\n        scrolled_height = driver.execute_script(\"return window.pageYOffset;\")\n        message(f\"Current scroll position: {scrolled_height}/{total_height}\")  # Mostra a posição atual e o tamanho máximo",
        "detail": "src.lib.extract.selenium_service",
        "documentation": {}
    },
    {
        "label": "test_connection",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def test_connection() -> None:\n    \"\"\"\n    Testa a conexão com a API da Shopify.\n    \"\"\"\n    url = f\"{BASE_URL}products.json\"\n    response = requests.get(url, headers=HEADERS)\n    if response.status_code == 200:\n        message(\"Conexão bem-sucedida! A API está acessível.\")\n        return True\n    else:",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "format_product_for_shopify",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def format_product_for_shopify(row: pd.Series) -> Tuple[dict, dict]:\n    \"\"\"\n    Formata os dados de um produto para o formato esperado pela API da Shopify.\n    Returns:\n    - product_data: dict com dados do produto\n    - variant_data: dict com dados da variante\n    \"\"\"\n    try:\n        product_type = row['product_def_pred_tag'] if pd.notna(row['product_def_pred_tag']) else row['product_def_tag']\n        button_html = f'''",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "get_all_skus_with_product_ids",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def get_all_skus_with_product_ids() -> dict:\n    \"\"\"\n    Obtém todas as SKUs de todos os produtos da Shopify, incluindo o product_id.\n    Returns:\n    - dict: Um dicionário onde as chaves são SKUs e os valores são listas de dicionários\n            contendo 'variant_id' e 'product_id'.\n    \"\"\"\n    try:\n        session = requests.Session()\n        session.headers.update(HEADERS)",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "find_duplicate_skus",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def find_duplicate_skus(sku_data: dict) -> dict:\n    \"\"\"\n    Encontra SKUs duplicadas no dicionário de SKUs.\n    Args:\n    - sku_data (dict): O dicionário retornado pela função get_all_skus_with_product_ids.\n    Returns:\n    - dict: Um dicionário onde as chaves são SKUs duplicadas e os valores são listas\n            de variantes (com 'variant_id' e 'product_id') associadas a essa SKU.\n    \"\"\"\n    duplicate_skus = {}",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "delete_duplicates_products",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def delete_duplicates_products(duplicate_skus):\n    if duplicate_skus:\n        message(f\"Encontrado {len(duplicate_skus)} SKUs duplicadas.\")\n        # Deletar todos os produtos e variantes encontrados\n        for sku, variants in duplicate_skus.items():\n            for variant in variants:\n                product_id = variant['product_id']\n                variant_id = variant['variant_id']\n                # Verifica o número de variantes restantes no produto\n                session = requests.Session()",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "get_variant_count",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def get_variant_count(session, product_id):\n    \"\"\"\n    Obtém o número de variantes de um produto.\n    Args:\n    - session: Sessão de requisição autenticada.\n    - product_id: ID do produto.\n    Returns:\n    - int: Número de variantes do produto.\n    \"\"\"\n    response = session.get(f\"{BASE_URL}products/{product_id}.json\", params={\"fields\": \"variants\"})",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "delete_variant",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def delete_variant(session, variant_id):\n    \"\"\"\n    Deleta uma variante específica.\n    Args:\n    - session: Sessão de requisição autenticada.\n    - variant_id: ID da variante a ser deletada.\n    \"\"\"\n    response = session.delete(f\"{BASE_URL}variants/{variant_id}.json\")\n    if response.status_code != 200:\n        message(f\"Erro ao deletar variante {variant_id}: {response.status_code} - {response.text}\")",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "delete_product",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def delete_product(session, product_id):\n    \"\"\"\n    Deleta um produto específico.\n    Args:\n    - session: Sessão de requisição autenticada.\n    - product_id: ID do produto a ser deletado.\n    \"\"\"\n    response = session.delete(f\"{BASE_URL}products/{product_id}.json\")\n    if response.status_code != 200:\n        message(f\"Erro ao deletar produto {product_id}: {response.status_code} - {response.text}\")",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "update_product_by_sku",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def update_product_by_sku(sku: str, product_data: dict, variant_data: dict, row: pd.Series, sku_data: dict) -> bool:\n    session = requests.Session()\n    session.headers.update(HEADERS)\n    if sku in sku_data:\n        variants = sku_data[sku]\n        product_id = variants[0]['product_id']\n        variant_id = variants[0]['variant_id']\n        # Atualiza o produto\n        product_success = update_product(session, product_id, product_data)\n        # Atualiza a variante",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "update_product",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def update_product(session, product_id: int, product_data: dict) -> bool:\n    url = f\"{BASE_URL}products/{product_id}.json\"\n    product_data['id'] = product_id\n    response = session.put(url, json={\"product\": product_data})\n    if response.status_code == 200:\n        message(f\"Produto {product_id} atualizado com sucesso.\")\n        return True\n    else:\n        error_message = response.json().get('errors', response.text)\n        message(f\"Erro ao atualizar o produto {product_id}: {response.status_code} - {error_message}\")",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "update_images",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def update_images(session, product_id: int, row: pd.Series) -> bool:\n    \"\"\"\n    Atualiza as imagens de um produto na Shopify.\n    \"\"\"\n    try:\n        # Obtém as imagens atuais do produto\n        response = session.get(f\"{BASE_URL}products/{product_id}/images.json\")\n        if response.status_code != 200:\n            message(f\"Erro ao obter imagens do produto {product_id}: {response.status_code} - {response.text}\")\n            return False",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "update_collections",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def update_collections(session, product_id: int, row: pd.Series) -> bool:\n    \"\"\"\n    Adiciona o produto a uma coleção manual na Shopify.\n    \"\"\"\n    try:\n        collection_title = row['collection'] if pd.notna(row['collection']) else \"Sem coleção\"\n        # Obtém todas as coleções manuais (Custom Collections)\n        response = session.get(f\"{BASE_URL}custom_collections.json\", params={\"title\": collection_title})\n        if response.status_code != 200:\n            message(f\"Erro ao obter coleções: {response.status_code} - {response.text}\")",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "update_variant",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def update_variant(session, product_id: int, variant_id: int, variant_data: dict) -> bool:\n    url = f\"{BASE_URL}variants/{variant_id}.json\"\n    variant_data['id'] = variant_id\n    response = session.put(url, json={\"variant\": variant_data})\n    if response.status_code == 200:\n        message(f\"Variante {variant_id} do produto {product_id} atualizada com sucesso.\")\n        return True\n    else:\n        try:\n            error_message = response.json().get('errors', response.text)",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "create_product",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def create_product(product_data: dict):\n    \"\"\"\n    Cria um novo produto na Shopify com os dados fornecidos.\n    Args:\n    - product_data (dict): Dados formatados do produto para criação.\n    Returns:\n    - dict: O produto criado retornado pela API Shopify, ou None em caso de erro.\n    \"\"\"\n    session = requests.Session()\n    session.headers.update(HEADERS)",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "get_product_by_sku",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def get_product_by_sku(sku: str):\n    \"\"\"\n    Obtém o produto com base no SKU.\n    \"\"\"\n    session = requests.Session()\n    session.headers.update(HEADERS)\n    params = {\n        \"fields\": \"id,variants\",\n        \"limit\": 250,\n        \"variants.sku\": sku",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "process_and_ingest_products",
        "kind": 2,
        "importPath": "src.lib.load.connection.shopify",
        "description": "src.lib.load.connection.shopify",
        "peekOfCode": "def process_and_ingest_products(df: pd.DataFrame) -> None:\n    is_connected = test_connection()\n    if (not is_connected):\n        raise ValueError(f\"no connection with the Shopify\") \n    sku_data = get_all_skus_with_product_ids()\n    duplicate_skus = find_duplicate_skus(sku_data)\n    delete_duplicates_products(duplicate_skus)\n    # Atualiza sku_data após deletar duplicatas\n    sku_data = get_all_skus_with_product_ids()\n    for index, row in df.iterrows():",
        "detail": "src.lib.load.connection.shopify",
        "documentation": {}
    },
    {
        "label": "image_ingestion",
        "kind": 2,
        "importPath": "src.lib.load.image_ingestion",
        "description": "src.lib.load.image_ingestion",
        "peekOfCode": "def image_ingestion(df, conf):\n    global CONF\n    CONF = conf\n    file_path = CONF['data_path']\n    images_path = file_path + \"/img_csl/\"\n    dataindex_img_path = os.getenv('DATAINDEX_IMG_PATH')\n    images_server_path = dataindex_img_path + \"/imgs\"\n    git_pull(dataindex_img_path)\n    if not os.path.exists(images_server_path):\n        os.makedirs(images_server_path)",
        "detail": "src.lib.load.image_ingestion",
        "documentation": {}
    },
    {
        "label": "git_pull",
        "kind": 2,
        "importPath": "src.lib.load.image_ingestion",
        "description": "src.lib.load.image_ingestion",
        "peekOfCode": "def git_pull(project_dir):\n    try:\n        original_dir = os.getcwd()\n        os.chdir(project_dir)\n        subprocess.check_call(['git', 'checkout', '.'])\n        subprocess.check_call(['git', 'pull'])\n        print(\"Successful pull.\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error when running Git command: {e}\")\n    except Exception as e:",
        "detail": "src.lib.load.image_ingestion",
        "documentation": {}
    },
    {
        "label": "git_push",
        "kind": 2,
        "importPath": "src.lib.load.image_ingestion",
        "description": "src.lib.load.image_ingestion",
        "peekOfCode": "def git_push(project_dir):\n    original_dir = os.getcwd()\n    try:\n        os.chdir(project_dir)\n        if os.system('git add .') != 0:\n            print(\"Error staging files.\")\n            return\n        if os.system('git diff --cached --exit-code') == 0:\n            print(\"No changes to commit.\")\n            return",
        "detail": "src.lib.load.image_ingestion",
        "documentation": {}
    },
    {
        "label": "load",
        "kind": 2,
        "importPath": "src.lib.load.load",
        "description": "src.lib.load.load",
        "peekOfCode": "def load(conf):\n    # Carregar os DataFrames e adicionar a coluna 'transform'\n    df_products_transform_csl = pd.read_csv(conf['data_path'] + '/products_transform_csl.csv')\n    df_products_transform_csl['is_transform_data'] = 1\n    products_load_csl_path = conf['data_path'] + '/products_load_csl.csv'\n    df_products_load_csl = create_or_read_df(products_load_csl_path, df_products_transform_csl.columns)\n    if (not df_products_load_csl.empty):\n        df_products_load_csl['is_transform_data'] = 0\n        # Unir os DataFrames\n        df_union = pd.concat([df_products_transform_csl, df_products_load_csl])",
        "detail": "src.lib.load.load",
        "documentation": {}
    },
    {
        "label": "load_product_definition",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def load_product_definition(df, conf):\n    message(\"RUNNING MODEL PREP...\")\n    global CONF\n    global WORDLIST\n    global DATA_PATH\n    global FILE_PATH_PRODUCT_METADATA\n    CONF = conf\n    WORDLIST = conf[\"wordlist\"]\n    DATA_PATH = CONF[\"data_path\"]\n    FILE_PATH_PRODUCT_METADATA = f\"{DATA_PATH}/products_metadata_transform.csv\"",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "generates_and_stacks_product_metadata_by_ref",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def generates_and_stacks_product_metadata_by_ref(ref, keywords_data):\n    title_keywords = keywords_data[ref][0]\n    valid_keywords, others_keywords = treat_relationship_between_keywords(title_keywords)\n    excluded_title_keywords = keywords_data[ref][1:]\n    if valid_keywords:\n        for contained in (True, False):\n            dfs_temp = [create_product_metadata_dataframe(keywords, valid_keywords, ref, contained, index) for index, keywords in enumerate(excluded_title_keywords)]\n            df_product_ref_info = pd.concat(dfs_temp, ignore_index=True)\n            df_product_ref_info['target'] = int(contained)\n            append_new_df_and_save(FILE_PATH_PRODUCT_METADATA, df_product_ref_info)",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "create_product_metadata_dataframe",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def create_product_metadata_dataframe(keywords, subjects, ref, contained, index):\n    product_ref_info = [\n        {\n            \"ref\": ref, \n            \"back_word\": keyword['back_words'], \n            \"subject\": keyword['subject'],\n            \"location\": int(keyword['location']), \n            \"word_number\": int(keyword['word_number']),\n            \"document_size\": int(keyword['document_size']),\n            \"document_index\": int(index),",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "shift_words",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def shift_words(df, columns):\n    for col in columns:\n        if df[col].dtype != 'object':\n            df[col] = df[col].astype('object')\n    def shift_words_to_right(row):\n        words = [row[col] for col in columns]\n        filtered_words = [w for w in words if pd.notna(w)]\n        none_filled = [None] * (len(columns) - len(filtered_words))\n        return none_filled + filtered_words\n    for index, row in df.iterrows():",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "treat_relationship_between_keywords",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def treat_relationship_between_keywords(keywrods):\n    if (keywrods == {}):\n        return False, False\n    subjects_keywords = list(set([keyword['subject'] for keyword in keywrods.values()]))\n    valid_keywords = []\n    others_keywords = []\n    for subject_keywords in subjects_keywords:\n        subject = WORDLIST[subject_keywords]\n        if ((subject['product']) &\n            (not bool(set(subject['conflict']).intersection(subjects_keywords)))):",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "extract_keywords_from_products",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def extract_keywords_from_products(df, conf):\n    keywords_data = {}\n    document_from_tag_count = 3\n    for idx, row in df.iterrows():\n        ref = row['ref']\n        title = row['title']\n        message(f\"extract data from {ref} - {title}\")\n        page_path = f\"{DATA_PATH}/products/{ref}.txt\"\n        html_text = read_file(page_path)\n        document_from_tag_flag = True",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "get_keywords_info",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def get_keywords_info(document, index):\n    documents_accents = clean_text(document, False, False, False, False, True)\n    documents_cleaned = clean_text(document, False, False, False, True, True)\n    first_doc = False\n    if (index == 0):\n        first_doc = True\n    keywords_info = {}\n    for key, value in WORDLIST.items():\n        subject = value['subject']\n        for word in subject:",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "filter_unique_locations",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def filter_unique_locations(data):\n    sorted_items = sorted(data.items(), key=lambda x: (x[1]['location'], -x[1]['size_word']))\n    filtered_data = {}\n    last_location = -1\n    for key, value in sorted_items:\n        if value['location'] != last_location:\n            filtered_data[key] = value\n            last_location = value['location']\n    return filtered_data\ndef normalize_data(data, columns):",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "normalize_data",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def normalize_data(data, columns):\n    if isinstance(data, dict):\n        return normalize_dict_of_dicts(data, columns)\n    elif isinstance(data, list):\n        return normalize_list_of_dicts(data, columns)\n    else:\n        return None\ndef normalize_dict_of_dicts(data, columns):\n    records = list(data.values())\n    min_max_values = {}",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "normalize_dict_of_dicts",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def normalize_dict_of_dicts(data, columns):\n    records = list(data.values())\n    min_max_values = {}\n    for col in columns:\n        col_values = [record[col] for record in records if col in record]\n        if col_values:\n            min_max_values[col] = {\n                'min': min(col_values),\n                'max': max(col_values)\n            }",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "normalize_list_of_dicts",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def normalize_list_of_dicts(data, columns):\n    max_values = {col: max(entry[col] for entry in data) for col in columns}\n    min_values = {col: min(entry[col] for entry in data) for col in columns}\n    normalized_data = []\n    for entry in data:\n        normalized_entry = entry.copy()\n        for col in columns:\n            array = entry[col]\n            max_val = max_values[col]\n            min_val = min_values[col]",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "normalize_rows",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def normalize_rows(df, exclude_columns):\n    # Cria uma cópia do DataFrame para não modificar o original.\n    df_normalized = df.copy()\n    # Itera sobre as linhas do DataFrame.\n    for index, row in df.iterrows():\n        # Seleciona os valores das colunas que não estão na lista de exclusão.\n        values_to_normalize = row.drop(exclude_columns)\n        # Calcula o mínimo e o máximo dos valores selecionados.\n        min_val = values_to_normalize.min()\n        max_val = values_to_normalize.max()",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "extract_subject_from_html_text",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def extract_subject_from_html_text(html_text, tag_map=None):\n    soup = BeautifulSoup(html_text, 'html.parser')\n    text = \"\"\n    if (not tag_map):\n        remove_tags = ['header', 'footer', 'fieldset', 'select', 'script', 'style', 'iframe', 'svg', 'img', 'link', 'button', 'noscript']\n        for remove_tag in remove_tags:\n            for tag_map in soup.find_all(remove_tag):\n                tag_map.decompose()\n        tags_com_hidden = soup.find_all(lambda element: element.get('class') and 'hidden' in ' '.join(element.get('class')))\n        for tag_map in tags_com_hidden:",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "append_new_df_and_save",
        "kind": 2,
        "importPath": "src.lib.transform.product_definition",
        "description": "src.lib.transform.product_definition",
        "peekOfCode": "def append_new_df_and_save(path, new_df):\n    if path_exists(path):\n        existing_df = pd.read_csv(path)\n        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n    else:\n        combined_df = new_df\n    combined_df.to_csv(path, index=False)",
        "detail": "src.lib.transform.product_definition",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 2,
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "peekOfCode": "def transform(conf, df):\n    message(\"START TRANSFORM\")\n    message(\"Filtro de nulos\")\n    df = filter_nulls(df)\n    message(\"Aplicando filtros de nome|preço|marca\")\n    df = apply_generic_filters(df, conf)\n    message(\"Criando coluna quantidade\")\n    df = create_quantity_column(df)\n    message(\"Removendo produtos da blacklist\")\n    df = remove_blacklisted_products(df)",
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "create_product_collection_col",
        "kind": 2,
        "importPath": "src.lib.transform.transform",
        "description": "src.lib.transform.transform",
        "peekOfCode": "def create_product_collection_col(df):\n    import numpy as np\n    def assign_collection(row):\n        if (row['price_discount_percent'] > 0):\n            return 'Promoção'\n        elif ('whey' in str(row['product_def']).lower() or 'whey' in str(row['product_def_pred']).lower()):\n            return 'Whey Protein'\n        elif ('barrinha' in str(row['product_def']).lower() or 'barrinha' in str(row['product_def_pred']).lower()):\n            return 'Barrinhas'\n        elif ('creatina' in str(row['product_def']).lower() or 'creatina' in str(row['product_def_pred']).lower()):",
        "detail": "src.lib.transform.transform",
        "documentation": {}
    },
    {
        "label": "create_product_def_cols",
        "kind": 2,
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "peekOfCode": "def create_product_def_cols(df, conf):\n    message(\"criada colunas de definição do produto\")\n    product_definition = conf['product_definition']\n    message(\"Load_models_prep\")\n    load_product_definition(df, conf)\n    message(\"carregando colunas de definição\")\n    product_definition_by_titile = f\"{product_definition}/product_definition_by_titile.csv\"\n    product_definition_by_ml = f\"{product_definition}/product_definition_by_ml.csv\"\n    if ((path_exists(product_definition_by_titile)) & (path_exists(product_definition_by_ml))):\n        df_product_def = pd.read_csv(product_definition_by_titile)",
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "ensure_columns_exist",
        "kind": 2,
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "peekOfCode": "def ensure_columns_exist(df, columns):\n    for column in columns:\n        if column not in df.columns:\n            df[column] = np.nan  # Adiciona a coluna com valores NaN (nulos)\n    return df\ndef filter_nulls(df):\n    \"\"\"Filter rows with null values in specific columns and save them to a CSV file.\"\"\"\n    return df.dropna(subset=['title', 'price', 'image_url']).reset_index(drop=True)\ndef apply_generic_filters(df, conf):\n    \"\"\"Apply various data cleaning and transformation filters.\"\"\"",
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "filter_nulls",
        "kind": 2,
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "peekOfCode": "def filter_nulls(df):\n    \"\"\"Filter rows with null values in specific columns and save them to a CSV file.\"\"\"\n    return df.dropna(subset=['title', 'price', 'image_url']).reset_index(drop=True)\ndef apply_generic_filters(df, conf):\n    \"\"\"Apply various data cleaning and transformation filters.\"\"\"\n    df['title_extract'] = df['title']\n    df['name'] = df['title'].str.lower()\n    df['price'] = df['price'].str.replace('R$', '').str.replace(' ', '')\n    df['brand'] = conf['brand']\n    df['price_numeric'] = df['price'].str.replace(',', '.').astype(float)",
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "apply_generic_filters",
        "kind": 2,
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "peekOfCode": "def apply_generic_filters(df, conf):\n    \"\"\"Apply various data cleaning and transformation filters.\"\"\"\n    df['title_extract'] = df['title']\n    df['name'] = df['title'].str.lower()\n    df['price'] = df['price'].str.replace('R$', '').str.replace(' ', '')\n    df['brand'] = conf['brand']\n    df['price_numeric'] = df['price'].str.replace(',', '.').astype(float)\n    df['title'] = df['title'].apply(clean_text).apply(remove_spaces)\n    return df\ndef create_quantity_column(df):",
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "create_quantity_column",
        "kind": 2,
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "peekOfCode": "def create_quantity_column(df):\n    \"\"\"Extract and convert quantity information into a uniform format.\"\"\"\n    # Garantir que sempre retornamos dois elementos para evitar o erro de tamanho de coluna\n    df['quantity_unit'] = df['name'].apply(lambda text: find_pattern_for_quantity(text))\n    # Separar a coluna 'quantity_unit' em 'quantity' e 'unit'\n    df[['quantity', 'unit']] = pd.DataFrame(df['quantity_unit'].tolist(), index=df.index)\n    # Aplicar conversão para gramas\n    df['quantity'] = df[['quantity', 'unit']].apply(convert_to_grams, axis=1)\n    # Calcular o preço por quantidade\n    df['price_qnt'] = df.apply(relation_qnt_price, axis=1)",
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "remove_blacklisted_products",
        "kind": 2,
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "peekOfCode": "def remove_blacklisted_products(df):\n    \"\"\"Remove products based on a blacklist.\"\"\"\n    return df[~df['title'].apply(lambda x: find_in_text_with_wordlist(x, BLACK_LIST))]\ndef find_pattern_for_quantity(text):\n    pattern = r'(\\d+[.,]?\\d*)\\s*(kg|g|gr|gramas)'\n    matches = re.findall(pattern, text, re.IGNORECASE)\n    quantity, unit = None, None\n    if len(matches) == 1:\n        quantity, unit = matches[0]\n        quantity = str(quantity).replace(',', '.')",
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "find_pattern_for_quantity",
        "kind": 2,
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "peekOfCode": "def find_pattern_for_quantity(text):\n    pattern = r'(\\d+[.,]?\\d*)\\s*(kg|g|gr|gramas)'\n    matches = re.findall(pattern, text, re.IGNORECASE)\n    quantity, unit = None, None\n    if len(matches) == 1:\n        quantity, unit = matches[0]\n        quantity = str(quantity).replace(',', '.')\n        if unit in ['g', 'gr', 'gramas'] and \".\" in quantity:\n            quantity = quantity.replace(\".\", \"\")\n        quantity = float(quantity)",
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "convert_to_grams",
        "kind": 2,
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "peekOfCode": "def convert_to_grams(row):\n    value = row['quantity']\n    unit = row['unit']\n    if pd.notna(value):\n        if unit in ['kg']:\n            value = float(value) * 1000\n        try:\n            value = int(float(value))\n        except ValueError:\n            pass",
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "relation_qnt_price",
        "kind": 2,
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "peekOfCode": "def relation_qnt_price(row):\n    resultado = (row['price_numeric'] / row['quantity']) if (row['quantity'] > 0) else -1\n    if resultado < 0:\n        return np.nan\n    return round(resultado, 3)\ndef image_processing(df, data_path):\n    message(\"image_processing\")\n    path_img_tmp = data_path + \"/img_tmp/\"\n    path_img_hash = data_path + \"/img_hash/\"\n    path_img_csl = data_path + \"/img_csl/\"",
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "image_processing",
        "kind": 2,
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "peekOfCode": "def image_processing(df, data_path):\n    message(\"image_processing\")\n    path_img_tmp = data_path + \"/img_tmp/\"\n    path_img_hash = data_path + \"/img_hash/\"\n    path_img_csl = data_path + \"/img_csl/\"\n    create_directory_if_not_exists(path_img_hash)\n    create_directory_if_not_exists(path_img_csl)\n    refs = sorted(df['ref'])\n    dict_imgs = {i.split(\".\")[0]: i for i in list_directory(path_img_tmp) if i.split(\".\")[0] in refs}\n    dict_imgs = dict(sorted(dict_imgs.items(), key=lambda item: item[1]))",
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "filter_df_price_when_alter_price",
        "kind": 2,
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "peekOfCode": "def filter_df_price_when_alter_price(df, refs):\n    for ref in refs:\n        df_temp = df[df[\"ref\"] == ref].sort_values('ing_date')\n        last_price = False\n        for idx, row in df_temp.iterrows():\n            price = row['price_numeric']\n            is_alter_price = False\n            if ((not last_price) | bool(last_price != price)):\n                last_price = price\n                is_alter_price = True",
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "create_price_discount_percent_col",
        "kind": 2,
        "importPath": "src.lib.transform.transform_functions",
        "description": "src.lib.transform.transform_functions",
        "peekOfCode": "def create_price_discount_percent_col(df, data_path):\n    df_new = deepcopy(df)\n    refs = df['ref'].values\n    path = f\"{data_path}/history\"\n    df_temp = read_and_stack_historical_csvs_dataframes(path, False)\n    if (df_temp.empty):\n        df[\"price_discount_percent\"] = 0\n        df[\"compare_at_price\"] = None\n        return df\n    df_temp = df_temp[df_temp[\"ref\"].isin(refs)]",
        "detail": "src.lib.transform.transform_functions",
        "documentation": {}
    },
    {
        "label": "status_tag",
        "kind": 2,
        "importPath": "src.lib.utils.data_quality",
        "description": "src.lib.utils.data_quality",
        "peekOfCode": "def status_tag(data, kill_job=True):\n    errors = []\n    if not is_price(data[\"price\"]):\n        errors.append(\"ERRO: Invalid price format.\")\n    if not check_url_existence(data[\"image_url\"]):\n        errors.append(\"ERRO: Image URL does not exist.\")\n    if not check_url_existence(data[\"product_url\"]):\n        errors.append(\"ERRO: Product URL does not exist.\")\n    if errors:\n        for error in errors:",
        "detail": "src.lib.utils.data_quality",
        "documentation": {}
    },
    {
        "label": "data_history_analysis",
        "kind": 2,
        "importPath": "src.lib.utils.data_quality",
        "description": "src.lib.utils.data_quality",
        "peekOfCode": "def data_history_analysis(conf, df):\n    history_path = conf['data_path'] + \"/history\"\n    create_directory_if_not_exists(history_path)\n    if (not has_files(history_path)):\n        save_history_data(conf, df)\n        return True\n    message(\"CARREGANDO DATAFRAME HISTORICO...\")\n    message(history_path)\n    df_history = read_and_stack_historical_csvs_dataframes(history_path, True)\n    message(\"ANALISE volume\")",
        "detail": "src.lib.utils.data_quality",
        "documentation": {}
    },
    {
        "label": "volume_analysis",
        "kind": 2,
        "importPath": "src.lib.utils.data_quality",
        "description": "src.lib.utils.data_quality",
        "peekOfCode": "def volume_analysis(df_history, df, alert_threshold=0.1, error_threshold=0.2):\n    volume_history = len(df_history)\n    volume_current = len(df)\n    if volume_history == 0:\n        return True, True\n    volume_change = abs((volume_current / volume_history) - 1)\n    message(f\"volume_change: {volume_change}\")\n    volume_error = volume_change < error_threshold\n    volume_alert = volume_change < alert_threshold\n    return volume_error, volume_alert",
        "detail": "src.lib.utils.data_quality",
        "documentation": {}
    },
    {
        "label": "title_analysis",
        "kind": 2,
        "importPath": "src.lib.utils.data_quality",
        "description": "src.lib.utils.data_quality",
        "peekOfCode": "def title_analysis(df_history, df):\n    df_history_title = df_history[[\"ref\", \"title\"]]\n    df_title = df[[\"ref\", \"title\"]]\n    result_df_title = df_history_title.merge(df_title, on='ref', how='inner')\n    if (result_df_title.empty):\n        message(\"ERRO RESULT_DF_TITLE.EMPTY\")\n        return False, False, df\n    result_df_title['diff_percent'] = result_df_title.apply(lambda row: calc_string_diff_in_df_col(row['title_x'], row['title_y']), axis=1).astype(float)\n    threshold_erro = 0.60\n    threshold_alert = 0.20",
        "detail": "src.lib.utils.data_quality",
        "documentation": {}
    },
    {
        "label": "price_analysis",
        "kind": 2,
        "importPath": "src.lib.utils.data_quality",
        "description": "src.lib.utils.data_quality",
        "peekOfCode": "def price_analysis(df_history, df):\n    df_history_price = df_history.groupby('ref')['price_numeric'].agg(['mean', 'max', 'min']).reset_index()\n    df_price = df[['ref', 'price_numeric']]\n    result_df_price = df_history_price.merge(df_price, on='ref', how='inner')\n    result_df_price['diff_percent'] = abs((result_df_price['price_numeric'] / result_df_price['mean']) - 1)\n    threshold_erro = 0.70\n    threshold_alert = 0.40\n    result_df_price['price_erro'] = result_df_price['diff_percent'] <= threshold_erro\n    result_df_price['price_alert'] = result_df_price['diff_percent'] <= threshold_alert\n    df_erro = result_df_price.sort_values('diff_percent')",
        "detail": "src.lib.utils.data_quality",
        "documentation": {}
    },
    {
        "label": "save_history_data",
        "kind": 2,
        "importPath": "src.lib.utils.data_quality",
        "description": "src.lib.utils.data_quality",
        "peekOfCode": "def save_history_data(conf, df):\n    history_path = conf['data_path'] + \"/history\"\n    data_atual = date.today()\n    formatted_date = data_atual.strftime(DATE_FORMAT)\n    create_directory_if_not_exists(history_path)\n    df.to_csv(f\"{history_path}/products_load_csl_{formatted_date}.csv\", index=False)\n    message(f\"Saved historical data in {history_path}/products_load_csl_{formatted_date}.csv\")\ndef is_price(string):\n    if not isinstance(string, str):\n        return False",
        "detail": "src.lib.utils.data_quality",
        "documentation": {}
    },
    {
        "label": "is_price",
        "kind": 2,
        "importPath": "src.lib.utils.data_quality",
        "description": "src.lib.utils.data_quality",
        "peekOfCode": "def is_price(string):\n    if not isinstance(string, str):\n        return False\n    pattern = r\"\"\"\n    (R\\$\\s?\\d{1,3}(?:\\.\\d{3})*[,.]\\d{2})|  # BRL: R$\n    (€\\s?\\d{1,3}(?:\\.\\d{3})*,\\d{2})|       # EUR: €\n    (\\$\\s?\\d{1,3}(?:,\\d{3})*\\.\\d{2})|      # USD: $\n    (£\\s?\\d{1,3}(?:,\\d{3})*\\.\\d{2})        # GBP: £\n    \"\"\"\n    return bool(re.match(pattern, string, re.VERBOSE))",
        "detail": "src.lib.utils.data_quality",
        "documentation": {}
    },
    {
        "label": "format_column_date",
        "kind": 2,
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "peekOfCode": "def format_column_date(df, column):\n    df[column] = pd.to_datetime(df[column], format=DATE_FORMAT, dayfirst=True)\n    df[column] = df[column].dt.strftime(DATE_FORMAT)\n    return df\ndef create_or_read_df(path, columns):\n    message(f\"create_or_read_df\")\n    if (path_exists(path)):\n        message(f\"read file: {path}\")\n        df = pd.read_csv(path)\n    else:",
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "create_or_read_df",
        "kind": 2,
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "peekOfCode": "def create_or_read_df(path, columns):\n    message(f\"create_or_read_df\")\n    if (path_exists(path)):\n        message(f\"read file: {path}\")\n        df = pd.read_csv(path)\n    else:\n        df = pd.DataFrame(columns=columns)\n        message(f\"create file: {path}\")\n        df.to_csv(path, index=False)\n    return df",
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "filter_dataframe_for_columns",
        "kind": 2,
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "peekOfCode": "def filter_dataframe_for_columns(df: pd.DataFrame, columns: List[str], keywords: List[str], blacklist: Optional[List[str]] = None) -> pd.DataFrame:\n    \"\"\"Filters a DataFrame for specified columns based on keywords and an optional blacklist to exclude certain terms\"\"\"\n    global_mask = pd.Series([False] * len(df), index=df.index)\n    for col in columns:\n        df[col] = df[col].astype(str).fillna('')\n        global_mask |= df[col].str.contains('|'.join(keywords), case=False)\n    filtered_df = df[global_mask]\n    if blacklist:\n        for col in columns:\n            blacklist_mask = ~filtered_df[col].str.contains('|'.join(blacklist), case=False)",
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "drop_duplicates_for_columns",
        "kind": 2,
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "peekOfCode": "def drop_duplicates_for_columns(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"Drop duplicates based on specific columns\"\"\"\n    return df.drop_duplicates(subset=columns)\ndef calc_string_diff_in_df_col(title_x, title_y):\n    distance = levenshtein(title_x, title_y)\n    max_len = max(len(title_x), len(title_y))\n    percent_diff = (distance / max_len) if max_len != 0 else 0\n    return percent_diff\ndef read_and_stack_historical_csvs_dataframes(history_data_path, get_only_last):\n    # Usa glob para encontrar todos os arquivos CSV no diretório",
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "calc_string_diff_in_df_col",
        "kind": 2,
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "peekOfCode": "def calc_string_diff_in_df_col(title_x, title_y):\n    distance = levenshtein(title_x, title_y)\n    max_len = max(len(title_x), len(title_y))\n    percent_diff = (distance / max_len) if max_len != 0 else 0\n    return percent_diff\ndef read_and_stack_historical_csvs_dataframes(history_data_path, get_only_last):\n    # Usa glob para encontrar todos os arquivos CSV no diretório\n    csv_files = glob(os.path.join(history_data_path, '*.csv'))\n    csv_files = sorted(csv_files, reverse=True)\n    if get_only_last and csv_files:",
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "read_and_stack_historical_csvs_dataframes",
        "kind": 2,
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "peekOfCode": "def read_and_stack_historical_csvs_dataframes(history_data_path, get_only_last):\n    # Usa glob para encontrar todos os arquivos CSV no diretório\n    csv_files = glob(os.path.join(history_data_path, '*.csv'))\n    csv_files = sorted(csv_files, reverse=True)\n    if get_only_last and csv_files:\n        # Encontra o arquivo CSV mais recentemente modificado\n        latest_file = csv_files[0]\n        message(latest_file)\n        return pd.read_csv(latest_file)\n    elif csv_files:",
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "read_and_stack_csvs_dataframes",
        "kind": 2,
        "importPath": "src.lib.utils.dataframe",
        "description": "src.lib.utils.dataframe",
        "peekOfCode": "def read_and_stack_csvs_dataframes(data_path: str, pages: list, file_name: str) -> pd.DataFrame:\n    pages_path = [f\"{data_path}/{page}\" for page in pages]\n    df_temp = []\n    for path in pages_path:\n        file_path = f\"{path}/{file_name}\"\n        if os.path.exists(file_path):\n            df_temp.append(pd.read_csv(file_path))\n        else:\n            print(f\"Arquivo {file_path} não encontrado, pulando para o próximo.\")\n    if df_temp:",
        "detail": "src.lib.utils.dataframe",
        "documentation": {}
    },
    {
        "label": "save_file",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def save_file(text, path):\n    with open(path, \"w\") as file:\n        message(\"file path: \" + path)\n        file.write(str(text))\ndef read_file(file_path):\n    \"\"\"Reads a file and returns its contents as a string.\"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            return file.read()\n    except FileNotFoundError:",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "read_file",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def read_file(file_path):\n    \"\"\"Reads a file and returns its contents as a string.\"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(\"The file was not found.\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred: {e}\")",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "read_json",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def read_json(file_path: str) -> Optional[Any]:\n    \"\"\"Reads a JSON file and returns its content or None in case of an error.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            return json.load(file)\n    except FileNotFoundError:\n        message(f\"The file {file_path} was not found.\")\n    except json.JSONDecodeError:\n        message(f\"Error decoding the JSON file {file_path}.\")\n    except Exception as e:",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "save_json",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def save_json(file_name: str, data: Any) -> None:\n    \"\"\"Saves data to a JSON file.\"\"\"\n    with open(file_name, 'w', encoding='utf-8') as file:\n        json.dump(data, file, ensure_ascii=False)\ndef delete_file(file_path: str) -> None:\n    \"\"\"Deletes a file if it exists, logging the outcome.\"\"\"\n    try:\n        os.remove(file_path)\n        message(f\"File {file_path} has been deleted successfully\")\n    except FileNotFoundError:",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "delete_file",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def delete_file(file_path: str) -> None:\n    \"\"\"Deletes a file if it exists, logging the outcome.\"\"\"\n    try:\n        os.remove(file_path)\n        message(f\"File {file_path} has been deleted successfully\")\n    except FileNotFoundError:\n        message(f\"The file {file_path} does not exist\")\n    except Exception as e:\n        message(f\"An error occurred: {e}\")\ndef path_exists(path: str) -> bool:",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "path_exists",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def path_exists(path: str) -> bool:\n    \"\"\"Checks if a path exists.\"\"\"\n    message(f\"check - {path}\")\n    return os.path.exists(path)\ndef create_file_if_not_exists(file_path: str, text: Optional[str] = None) -> None:\n    \"\"\"Creates a file if it doesn't exist. Optionally writes text to it.\"\"\"\n    if not path_exists(file_path):\n        try:\n            with open(file_path, mode='a', encoding='utf-8') as file:\n                if text:",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "create_file_if_not_exists",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def create_file_if_not_exists(file_path: str, text: Optional[str] = None) -> None:\n    \"\"\"Creates a file if it doesn't exist. Optionally writes text to it.\"\"\"\n    if not path_exists(file_path):\n        try:\n            with open(file_path, mode='a', encoding='utf-8') as file:\n                if text:\n                    file.write(text + \"\\n\")\n                    message(f\"write '{text}' successfully.\")\n                message(f\"File '{file_path}' created successfully.\")\n        except FileExistsError:",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "create_directory_if_not_exists",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def create_directory_if_not_exists(directory_path):\n    if not path_exists(directory_path):\n        try:\n            os.makedirs(directory_path)\n            message(f\"Directory '{directory_path}' created successfully.\")\n        except OSError as error:\n            message(f\"Error creating directory '{directory_path}': {error}\")\ndef download_image(image_url, image_path, image_name):\n    message(f\"Download: {image_url}\")\n    response = requests.get(image_url)",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "download_image",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def download_image(image_url, image_path, image_name):\n    message(f\"Download: {image_url}\")\n    response = requests.get(image_url)\n    if response.status_code == 200:\n        # Obtendo o tipo de conteúdo da resposta\n        content_type = response.headers['Content-Type']\n        # Determinando a extensão com base no tipo de conteúdo\n        if 'image/jpeg' in content_type:\n            extension = '.jpg'\n        elif 'image/png' in content_type:",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "save_images",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def save_images(image_urls, image_path, image_names):\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        # Mapeia cada tarefa futura para a respectiva URL usando um dicionário\n        future_to_url = {executor.submit(download_image, url, image_path, name): url for url, name in zip(image_urls, image_names)}\n        # Itera sobre as tarefas concluídas conforme elas são finalizadas\n        for future in as_completed(future_to_url):\n            url = future_to_url[future]\n            try:\n                result = future.result()  # Obtém o resultado da tarefa\n                if result:",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "file_exists",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def file_exists(directory, filename):\n    file_path = os.path.join(directory, filename)\n    message(file_path)\n    return os.path.exists(file_path)\ndef delete_directory_and_contents(directory_path):\n    if not os.path.exists(directory_path):\n        message(\"Directory does not exist.\")\n        return\n    shutil.rmtree(directory_path)\n    message(f\"Directory and all contents deleted: {directory_path}\")",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "delete_directory_and_contents",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def delete_directory_and_contents(directory_path):\n    if not os.path.exists(directory_path):\n        message(\"Directory does not exist.\")\n        return\n    shutil.rmtree(directory_path)\n    message(f\"Directory and all contents deleted: {directory_path}\")\ndef get_old_files_by_percent(directory_path, sort_ascending=True, percentage=5):\n    all_files = [file for file in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, file))]\n    files_info = []\n    for file in all_files:",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "get_old_files_by_percent",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def get_old_files_by_percent(directory_path, sort_ascending=True, percentage=5):\n    all_files = [file for file in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, file))]\n    files_info = []\n    for file in all_files:\n        file_path = os.path.join(directory_path, file)\n        last_modification_time = os.path.getmtime(file_path)\n        last_modification_date = datetime.datetime.fromtimestamp(last_modification_time)\n        files_info.append((file, last_modification_date))\n    files_info.sort(key=lambda x: x[1], reverse=not sort_ascending)\n    files_count = len(files_info)",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "list_directory",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def list_directory(path):\n    try:\n        # Check if the path is a valid directory\n        if os.path.isdir(path):\n            contents = os.listdir(path)\n            message(f\"Contents of directory '{path}':\")\n            items = []\n            for item in contents:\n                items.append(item)\n            return items",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "has_files",
        "kind": 2,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "def has_files(directory):\n    items = os.listdir(directory)\n    for item in items:\n        item_path = os.path.join(directory, item)\n        if os.path.isfile(item_path):\n            return True\n    return False",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "DATE_FORMAT",
        "kind": 5,
        "importPath": "src.lib.utils.file_system",
        "description": "src.lib.utils.file_system",
        "peekOfCode": "DATE_FORMAT = \"%Y-%m-%d\"\ndef save_file(text, path):\n    with open(path, \"w\") as file:\n        message(\"file path: \" + path)\n        file.write(str(text))\ndef read_file(file_path):\n    \"\"\"Reads a file and returns its contents as a string.\"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            return file.read()",
        "detail": "src.lib.utils.file_system",
        "documentation": {}
    },
    {
        "label": "get_pages_with_status_true",
        "kind": 2,
        "importPath": "src.lib.utils.general_functions",
        "description": "src.lib.utils.general_functions",
        "peekOfCode": "def get_pages_with_status_true(conf, return_job_name=True):\n    pages = list_directory(conf[\"pages_path\"])\n    pages_data_path = []\n    for page in pages:\n        module_name = f\"src.jobs.slave_page.pages.{conf['country']}.{page}.conf\"\n        page_conf = importlib.import_module(module_name)\n        if page_conf.STATUS:\n            value = page_conf.JOB_NAME if return_job_name else page_conf.BRAND\n            pages_data_path.append(value)\n    return sorted(pages_data_path)",
        "detail": "src.lib.utils.general_functions",
        "documentation": {}
    },
    {
        "label": "convert_image",
        "kind": 2,
        "importPath": "src.lib.utils.image_functions",
        "description": "src.lib.utils.image_functions",
        "peekOfCode": "def convert_image(image_path, save_path, output_format='webp'):\n    if (not os.path.isfile(image_path)):\n        raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n    with Image.open(image_path) as img:\n        img.save(save_path + '.' + output_format, output_format.upper())\ndef calculate_precise_image_hash(image_path):\n    with open(image_path, \"rb\") as image_file:\n        image_data = image_file.read()\n        image_hash = hashlib.sha256(image_data).hexdigest()\n    return image_hash",
        "detail": "src.lib.utils.image_functions",
        "documentation": {}
    },
    {
        "label": "calculate_precise_image_hash",
        "kind": 2,
        "importPath": "src.lib.utils.image_functions",
        "description": "src.lib.utils.image_functions",
        "peekOfCode": "def calculate_precise_image_hash(image_path):\n    with open(image_path, \"rb\") as image_file:\n        image_data = image_file.read()\n        image_hash = hashlib.sha256(image_data).hexdigest()\n    return image_hash",
        "detail": "src.lib.utils.image_functions",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "kind": 2,
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "peekOfCode": "def setup_logging(log_file: str = 'app.log') -> None:\n    \"\"\"\n    Configura o sistema de logging para registrar mensagens em um arquivo e no console.\n    Args:\n        log_file (str): Caminho para o arquivo de log. Padrão é 'app.log'.\n    \"\"\"\n    # Configuração básica para o arquivo de log\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',",
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "message",
        "kind": 2,
        "importPath": "src.lib.utils.log",
        "description": "src.lib.utils.log",
        "peekOfCode": "def message(msg: Union[str, Dict], level: str = 'info') -> None:\n    \"\"\"\n    Registra uma mensagem no log. Aceita strings ou dicionários.\n    Args:\n        msg (Union[str, Dict]): Mensagem a ser registrada. Pode ser uma string ou um dicionário.\n        level (str): Nível de logging ('debug', 'info', 'warning', 'error', 'critical'). Padrão é 'info'.\n    \"\"\"\n    logger = logging.getLogger()\n    # Serializa dicionários para JSON\n    if isinstance(msg, dict):",
        "detail": "src.lib.utils.log",
        "documentation": {}
    },
    {
        "label": "flatten_list",
        "kind": 2,
        "importPath": "src.lib.utils.py_functions",
        "description": "src.lib.utils.py_functions",
        "peekOfCode": "def flatten_list(list_of_lists):\n    if list_of_lists is None:\n        return []\n    flattened_list = []\n    for element in list_of_lists:\n        if isinstance(element, list):\n            for subelement in element:\n                flattened_list.append(subelement)\n        else:\n            flattened_list.append(element)",
        "detail": "src.lib.utils.py_functions",
        "documentation": {}
    },
    {
        "label": "find_in_text_with_wordlist",
        "kind": 2,
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "peekOfCode": "def find_in_text_with_wordlist(text, wordlist):\n    match = None\n    for word in wordlist:\n        clean_word = clean_text(word)\n        text = clean_text(text)\n        match = re.search(clean_word, clean_text(text))  # Expressão regular para encontrar dígitos\n        if match:\n            break\n    if match:\n        return True",
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "levenshtein",
        "kind": 2,
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "peekOfCode": "def levenshtein(s1, s2):\n    if len(s1) < len(s2):\n        return levenshtein(s2, s1)\n    if len(s2) == 0:\n        return len(s1)\n    previous_row = range(len(s2) + 1)\n    for i, c1 in enumerate(s1):\n        current_row = [i + 1]\n        for j, c2 in enumerate(s2):\n            insertions = previous_row[j + 1] + 1",
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "encode_to_base64",
        "kind": 2,
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "peekOfCode": "def encode_to_base64(value: str) -> str:\n    \"\"\"Encodes a string to Base64.\"\"\"\n    return base64.b64encode(value.encode('utf-8')).decode('utf-8')\ndef generate_numeric_hash(data: str) -> int:\n    \"\"\"Generates a numeric hash value for the given data.\"\"\"\n    hash_value = hash(data)\n    return abs(hash_value)\ndef generate_hash(value):\n    return hashlib.sha256(value.encode()).hexdigest()[:8]\ndef remove_spaces(text):",
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "generate_numeric_hash",
        "kind": 2,
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "peekOfCode": "def generate_numeric_hash(data: str) -> int:\n    \"\"\"Generates a numeric hash value for the given data.\"\"\"\n    hash_value = hash(data)\n    return abs(hash_value)\ndef generate_hash(value):\n    return hashlib.sha256(value.encode()).hexdigest()[:8]\ndef remove_spaces(text):\n    return re.sub(r'\\s+', ' ', text).strip()\ndef clean_string_break_line(value):\n    return remove_spaces(str(value).replace('\\n', '').replace('\\xa0', ' '))",
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "generate_hash",
        "kind": 2,
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "peekOfCode": "def generate_hash(value):\n    return hashlib.sha256(value.encode()).hexdigest()[:8]\ndef remove_spaces(text):\n    return re.sub(r'\\s+', ' ', text).strip()\ndef clean_string_break_line(value):\n    return remove_spaces(str(value).replace('\\n', '').replace('\\xa0', ' '))\ndef clean_text(text: str, clean_spaces: bool = False, remove_final_s: bool = False, \n               remove_break_line: bool = True, remove_accents: bool = True, \n               add_space_first: bool = False) -> Optional[str]:\n    \"\"\"Cleans and formats text based on the provided parameters.\"\"\"",
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "remove_spaces",
        "kind": 2,
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "peekOfCode": "def remove_spaces(text):\n    return re.sub(r'\\s+', ' ', text).strip()\ndef clean_string_break_line(value):\n    return remove_spaces(str(value).replace('\\n', '').replace('\\xa0', ' '))\ndef clean_text(text: str, clean_spaces: bool = False, remove_final_s: bool = False, \n               remove_break_line: bool = True, remove_accents: bool = True, \n               add_space_first: bool = False) -> Optional[str]:\n    \"\"\"Cleans and formats text based on the provided parameters.\"\"\"\n    if not isinstance(text, str):\n        return None",
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "clean_string_break_line",
        "kind": 2,
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "peekOfCode": "def clean_string_break_line(value):\n    return remove_spaces(str(value).replace('\\n', '').replace('\\xa0', ' '))\ndef clean_text(text: str, clean_spaces: bool = False, remove_final_s: bool = False, \n               remove_break_line: bool = True, remove_accents: bool = True, \n               add_space_first: bool = False) -> Optional[str]:\n    \"\"\"Cleans and formats text based on the provided parameters.\"\"\"\n    if not isinstance(text, str):\n        return None\n    if remove_accents:\n        text = ''.join(c for c in unicodedata.normalize('NFKD', text) if not unicodedata.combining(c))",
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "kind": 2,
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "peekOfCode": "def clean_text(text: str, clean_spaces: bool = False, remove_final_s: bool = False, \n               remove_break_line: bool = True, remove_accents: bool = True, \n               add_space_first: bool = False) -> Optional[str]:\n    \"\"\"Cleans and formats text based on the provided parameters.\"\"\"\n    if not isinstance(text, str):\n        return None\n    if remove_accents:\n        text = ''.join(c for c in unicodedata.normalize('NFKD', text) if not unicodedata.combining(c))\n    text = re.sub(r'[^\\w\\s]', ' ', text)\n    if remove_break_line:",
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "find_in_text_with_wordlist",
        "kind": 2,
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "peekOfCode": "def find_in_text_with_wordlist(text, wordlist):\n    match = None\n    for word in wordlist:\n        clean_word = clean_text(word)\n        text = clean_text(text)\n        match = re.search(clean_word, clean_text(text))  # Expressão regular para encontrar dígitos\n        if match:\n            break\n    if match:\n        return True",
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "DATE_FORMAT",
        "kind": 5,
        "importPath": "src.lib.utils.text_functions",
        "description": "src.lib.utils.text_functions",
        "peekOfCode": "DATE_FORMAT = \"%Y-%m-%d\"\ndef find_in_text_with_wordlist(text, wordlist):\n    match = None\n    for word in wordlist:\n        clean_word = clean_text(word)\n        text = clean_text(text)\n        match = re.search(clean_word, clean_text(text))  # Expressão regular para encontrar dígitos\n        if match:\n            break\n    if match:",
        "detail": "src.lib.utils.text_functions",
        "documentation": {}
    },
    {
        "label": "check_url_existence",
        "kind": 2,
        "importPath": "src.lib.utils.web_functions",
        "description": "src.lib.utils.web_functions",
        "peekOfCode": "def check_url_existence(url, timeout=5):\n    ua = UserAgent()\n    headers = {'User-Agent': ua.random}\n    try:\n        # Primeiro, tenta com o método HEAD\n        response = requests.head(url, headers=headers, timeout=timeout)\n        if response.status_code == 405:  # Método não permitido\n            # Tenta com GET se HEAD não for permitido\n            response = requests.get(url, headers=headers, timeout=timeout)\n        return 200 <= response.status_code < 400  # Inclui redirecionamentos",
        "detail": "src.lib.utils.web_functions",
        "documentation": {}
    },
    {
        "label": "get_dictionary",
        "kind": 2,
        "importPath": "src.lib.wordlist.brazil.dictionary",
        "description": "src.lib.wordlist.brazil.dictionary",
        "peekOfCode": "def get_dictionary(local):\n    conjugacoes = np.genfromtxt(f'{local}/src/lib/wordlist/brazil/conjugacoes.txt', dtype=str)\n    dicionario = np.genfromtxt(f'{local}/src/lib/wordlist/brazil/palavras.txt', dtype=str)\n    personal_pronouns = [\"Eu\", \"Tu\", \"Ele\", \"Ela\", \"Nós\", \"Vós\", \"Eles\", \"Elas\", \"Mim\", \"Ti\", \"Si\", \"Consigo\"]\n    oblique_pronouns = [\"Me\", \"Te\", \"Se\", \"Nos\", \"Vos\", \"O\", \"A\", \"Lhe\", \"Os\", \"As\", \"Nos\", \"Vos\", \"Se\", \"Convosco\", \"Lhes\", \"Contigo\"]\n    demonstrative_pronouns = [\"Este\", \"Esse\", \"Aquele\", \"Esta\", \"Essa\", \"Aquela\", \"Isto\", \"Isso\", \"Aquilo\", \"Estes\", \"Esses\", \"Aqueles\", \"Estas\", \"Essas\", \"Aquelas\", \"Iste\"]\n    possessive_pronouns = [\"Meu\", \"Teu\", \"Seu\", \"Nosso\", \"Vosso\", \"Seu\", \"Minha\", \"Tua\", \"Sua\", \"Nossa\", \"Vossa\", \"Sua\", \"Meus\", \"Teus\", \"Seus\", \"Nossos\", \"Vossos\", \"Minhas\", \"Tuas\", \"Suas\", \"Nossas\", \"Vossas\"]\n    indefinite_pronouns = [\"Alguém\", \"Ninguém\", \"Todo\", \"Algum\", \"Nenhum\", \"Outro\", \"Muito\", \"Pouco\", \"Tanto\", \"Cada\", \"Algo\", \"Tudo\", \"Nada\", \"Cada um\", \"Qualquer\", \"Poucos\", \"Muitos\", \"Vários\", \"Outrem\"]\n    relative_pronouns = [\"Que\", \"Qual\", \"Quem\", \"Onde\", \"Cujo\", \"O qual\", \"Cuja\", \"Quanto\"]\n    interrogative_pronouns = [\"Quem\", \"O que\", \"Qual\", \"Quanto\", \"Onde\", \"Quando\", \"Como\", \"Por que\", \"Qualquer coisa\", \"Quanto a\"]",
        "detail": "src.lib.wordlist.brazil.dictionary",
        "documentation": {}
    },
    {
        "label": "get_synonyms",
        "kind": 2,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "def get_synonyms(component_list):\n    keywords_list = []\n    for component, attributes in component_list.items():\n        keywords_list.append(attributes.get(\"subject\"))\n    return keywords_list\ndef get_word_index_in_text(word, text, add_space_firts):\n    text_temp = deepcopy(text)\n    word_clean = clean_text(word, False, False, False, True, False)\n    space = \" \" if not add_space_firts else \"\"\n    matches = re.finditer(space + word_clean, text_temp)",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "get_word_index_in_text",
        "kind": 2,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "def get_word_index_in_text(word, text, add_space_firts):\n    text_temp = deepcopy(text)\n    word_clean = clean_text(word, False, False, False, True, False)\n    space = \" \" if not add_space_firts else \"\"\n    matches = re.finditer(space + word_clean, text_temp)\n    locations = []\n    for match in matches:\n        start_index = match.start()\n        locations.append(start_index)\n    return locations",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "get_back_words",
        "kind": 2,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "def get_back_words(text_accents, locations):\n    size_max = 30\n    slice_min = lambda value: value if value >= 0 else 0\n    back_words = []\n    for location in locations:\n        start = slice_min(location - size_max)\n        back_words_aux = (text_accents[start:location].replace(\"\\n\", \"\"))\n        back_words_aux = [i for i in back_words_aux.split(\" \") if i != '']\n        back_words.append(back_words_aux)\n    return back_words",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "find_subject_in_wordlist",
        "kind": 2,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "def find_subject_in_wordlist(word, wordlist):\n    for values in wordlist.values():\n        subject = values['subject']\n        if (word in subject):\n            return subject\ndef remove_prepositions_pronouns(text, pronouns):\n    pronouns = set(pronouns)\n    for pronoun in pronouns:\n        space_pronoun = \" \" * len(pronoun)\n        text = text.replace(pronoun, space_pronoun)",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "remove_prepositions_pronouns",
        "kind": 2,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "def remove_prepositions_pronouns(text, pronouns):\n    pronouns = set(pronouns)\n    for pronoun in pronouns:\n        space_pronoun = \" \" * len(pronoun)\n        text = text.replace(pronoun, space_pronoun)\n    return text\nBLACK_LIST = [\n    \"regata\",\n    \"camiseta\",\n    \"coqueteleira\",",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "BLACK_LIST",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "BLACK_LIST = [\n    \"regata\",\n    \"camiseta\",\n    \"coqueteleira\",\n    \"mochila\",\n    \"garraf\",\n    \"bone\",\n    \"marmita\",\n    \"marmiteira\",\n    \"galao\",",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "SUPPLEMENT_COMPONENT_LIST",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "SUPPLEMENT_COMPONENT_LIST = {\n    \"nac\": {\n        \"subject\": [\n            \"acetil\",\n            \"nacetil\",\n            \"n acetil\",\n            \"cisteina\",\n            \"l cisteina\",\n            \"lcisteina\",\n            \"nac\",",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "COLLECTION",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "COLLECTION = {}\npersonal_pronouns = [\"Eu\", \"Tu\", \"Ele\", \"Ela\", \"Nós\", \"Vós\", \"Eles\", \"Elas\", \"Mim\", \"Ti\", \"Si\", \"Consigo\"]\noblique_pronouns = [\"Me\", \"Te\", \"Se\", \"Nos\", \"Vos\", \"O\", \"A\", \"Lhe\", \"Os\", \"As\", \"Nos\", \"Vos\", \"Se\", \"Convosco\", \"Lhes\", \"Contigo\"]\ndemonstrative_pronouns = [\"Este\", \"Esse\", \"Aquele\", \"Esta\", \"Essa\", \"Aquela\", \"Isto\", \"Isso\", \"Aquilo\", \"Estes\", \"Esses\", \"Aqueles\", \"Estas\", \"Essas\", \"Aquelas\", \"Iste\"]\npossessive_pronouns = [\"Meu\", \"Teu\", \"Seu\", \"Nosso\", \"Vosso\", \"Seu\", \"Minha\", \"Tua\", \"Sua\", \"Nossa\", \"Vossa\", \"Sua\", \"Meus\", \"Teus\", \"Seus\", \"Nossos\", \"Vossos\", \"Minhas\", \"Tuas\", \"Suas\", \"Nossas\", \"Vossas\"]\nindefinite_pronouns = [\"Alguém\", \"Ninguém\", \"Todo\", \"Algum\", \"Nenhum\", \"Outro\", \"Muito\", \"Pouco\", \"Tanto\", \"Cada\", \"Algo\", \"Tudo\", \"Nada\", \"Cada um\", \"Qualquer\", \"Poucos\", \"Muitos\", \"Vários\", \"Outrem\"]\nrelative_pronouns = [\"Que\", \"Qual\", \"Quem\", \"Onde\", \"Cujo\", \"O qual\", \"Cuja\", \"Quanto\"]\ninterrogative_pronouns = [\"Quem\", \"O que\", \"Qual\", \"Quanto\", \"Onde\", \"Quando\", \"Como\", \"Por que\", \"Qualquer coisa\", \"Quanto a\"]\nprepositions = [\"A\", \"Ante\", \"Até\", \"Após\", \"Com\", \"Contra\", \"De\", \"Desde\", \"Em\", \"Entre\", \"Para\", \"Por\", \"Perante\", \"Sem\", \"Sob\", \"Sobre\", \"Trás\", \"Conforme\", \"Contudo\", \"Durante\", \"Exceto\", \"Mediant\", \"Menos\", \"Salvo\", \"Segundo\", \"Visto\"]\nBRAZIL_PRONOUNS = personal_pronouns + oblique_pronouns + demonstrative_pronouns + possessive_pronouns + indefinite_pronouns + relative_pronouns + interrogative_pronouns + prepositions",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "personal_pronouns",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "personal_pronouns = [\"Eu\", \"Tu\", \"Ele\", \"Ela\", \"Nós\", \"Vós\", \"Eles\", \"Elas\", \"Mim\", \"Ti\", \"Si\", \"Consigo\"]\noblique_pronouns = [\"Me\", \"Te\", \"Se\", \"Nos\", \"Vos\", \"O\", \"A\", \"Lhe\", \"Os\", \"As\", \"Nos\", \"Vos\", \"Se\", \"Convosco\", \"Lhes\", \"Contigo\"]\ndemonstrative_pronouns = [\"Este\", \"Esse\", \"Aquele\", \"Esta\", \"Essa\", \"Aquela\", \"Isto\", \"Isso\", \"Aquilo\", \"Estes\", \"Esses\", \"Aqueles\", \"Estas\", \"Essas\", \"Aquelas\", \"Iste\"]\npossessive_pronouns = [\"Meu\", \"Teu\", \"Seu\", \"Nosso\", \"Vosso\", \"Seu\", \"Minha\", \"Tua\", \"Sua\", \"Nossa\", \"Vossa\", \"Sua\", \"Meus\", \"Teus\", \"Seus\", \"Nossos\", \"Vossos\", \"Minhas\", \"Tuas\", \"Suas\", \"Nossas\", \"Vossas\"]\nindefinite_pronouns = [\"Alguém\", \"Ninguém\", \"Todo\", \"Algum\", \"Nenhum\", \"Outro\", \"Muito\", \"Pouco\", \"Tanto\", \"Cada\", \"Algo\", \"Tudo\", \"Nada\", \"Cada um\", \"Qualquer\", \"Poucos\", \"Muitos\", \"Vários\", \"Outrem\"]\nrelative_pronouns = [\"Que\", \"Qual\", \"Quem\", \"Onde\", \"Cujo\", \"O qual\", \"Cuja\", \"Quanto\"]\ninterrogative_pronouns = [\"Quem\", \"O que\", \"Qual\", \"Quanto\", \"Onde\", \"Quando\", \"Como\", \"Por que\", \"Qualquer coisa\", \"Quanto a\"]\nprepositions = [\"A\", \"Ante\", \"Até\", \"Após\", \"Com\", \"Contra\", \"De\", \"Desde\", \"Em\", \"Entre\", \"Para\", \"Por\", \"Perante\", \"Sem\", \"Sob\", \"Sobre\", \"Trás\", \"Conforme\", \"Contudo\", \"Durante\", \"Exceto\", \"Mediant\", \"Menos\", \"Salvo\", \"Segundo\", \"Visto\"]\nBRAZIL_PRONOUNS = personal_pronouns + oblique_pronouns + demonstrative_pronouns + possessive_pronouns + indefinite_pronouns + relative_pronouns + interrogative_pronouns + prepositions\nBRAZIL_CONECTORES = ['a', 'o', 'e', 'ou', 'nem', 'mas', 'porque', 'como', 'apesar', 'além', 'entretanto', 'porém', 'todavia', 'logo', 'portanto', 'assim', 'contudo', 'embora', 'ainda', 'também', 'quer', 'seja', 'isto', 'aquilo']",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "oblique_pronouns",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "oblique_pronouns = [\"Me\", \"Te\", \"Se\", \"Nos\", \"Vos\", \"O\", \"A\", \"Lhe\", \"Os\", \"As\", \"Nos\", \"Vos\", \"Se\", \"Convosco\", \"Lhes\", \"Contigo\"]\ndemonstrative_pronouns = [\"Este\", \"Esse\", \"Aquele\", \"Esta\", \"Essa\", \"Aquela\", \"Isto\", \"Isso\", \"Aquilo\", \"Estes\", \"Esses\", \"Aqueles\", \"Estas\", \"Essas\", \"Aquelas\", \"Iste\"]\npossessive_pronouns = [\"Meu\", \"Teu\", \"Seu\", \"Nosso\", \"Vosso\", \"Seu\", \"Minha\", \"Tua\", \"Sua\", \"Nossa\", \"Vossa\", \"Sua\", \"Meus\", \"Teus\", \"Seus\", \"Nossos\", \"Vossos\", \"Minhas\", \"Tuas\", \"Suas\", \"Nossas\", \"Vossas\"]\nindefinite_pronouns = [\"Alguém\", \"Ninguém\", \"Todo\", \"Algum\", \"Nenhum\", \"Outro\", \"Muito\", \"Pouco\", \"Tanto\", \"Cada\", \"Algo\", \"Tudo\", \"Nada\", \"Cada um\", \"Qualquer\", \"Poucos\", \"Muitos\", \"Vários\", \"Outrem\"]\nrelative_pronouns = [\"Que\", \"Qual\", \"Quem\", \"Onde\", \"Cujo\", \"O qual\", \"Cuja\", \"Quanto\"]\ninterrogative_pronouns = [\"Quem\", \"O que\", \"Qual\", \"Quanto\", \"Onde\", \"Quando\", \"Como\", \"Por que\", \"Qualquer coisa\", \"Quanto a\"]\nprepositions = [\"A\", \"Ante\", \"Até\", \"Após\", \"Com\", \"Contra\", \"De\", \"Desde\", \"Em\", \"Entre\", \"Para\", \"Por\", \"Perante\", \"Sem\", \"Sob\", \"Sobre\", \"Trás\", \"Conforme\", \"Contudo\", \"Durante\", \"Exceto\", \"Mediant\", \"Menos\", \"Salvo\", \"Segundo\", \"Visto\"]\nBRAZIL_PRONOUNS = personal_pronouns + oblique_pronouns + demonstrative_pronouns + possessive_pronouns + indefinite_pronouns + relative_pronouns + interrogative_pronouns + prepositions\nBRAZIL_CONECTORES = ['a', 'o', 'e', 'ou', 'nem', 'mas', 'porque', 'como', 'apesar', 'além', 'entretanto', 'porém', 'todavia', 'logo', 'portanto', 'assim', 'contudo', 'embora', 'ainda', 'também', 'quer', 'seja', 'isto', 'aquilo']\nPRONOUNS = {",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "demonstrative_pronouns",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "demonstrative_pronouns = [\"Este\", \"Esse\", \"Aquele\", \"Esta\", \"Essa\", \"Aquela\", \"Isto\", \"Isso\", \"Aquilo\", \"Estes\", \"Esses\", \"Aqueles\", \"Estas\", \"Essas\", \"Aquelas\", \"Iste\"]\npossessive_pronouns = [\"Meu\", \"Teu\", \"Seu\", \"Nosso\", \"Vosso\", \"Seu\", \"Minha\", \"Tua\", \"Sua\", \"Nossa\", \"Vossa\", \"Sua\", \"Meus\", \"Teus\", \"Seus\", \"Nossos\", \"Vossos\", \"Minhas\", \"Tuas\", \"Suas\", \"Nossas\", \"Vossas\"]\nindefinite_pronouns = [\"Alguém\", \"Ninguém\", \"Todo\", \"Algum\", \"Nenhum\", \"Outro\", \"Muito\", \"Pouco\", \"Tanto\", \"Cada\", \"Algo\", \"Tudo\", \"Nada\", \"Cada um\", \"Qualquer\", \"Poucos\", \"Muitos\", \"Vários\", \"Outrem\"]\nrelative_pronouns = [\"Que\", \"Qual\", \"Quem\", \"Onde\", \"Cujo\", \"O qual\", \"Cuja\", \"Quanto\"]\ninterrogative_pronouns = [\"Quem\", \"O que\", \"Qual\", \"Quanto\", \"Onde\", \"Quando\", \"Como\", \"Por que\", \"Qualquer coisa\", \"Quanto a\"]\nprepositions = [\"A\", \"Ante\", \"Até\", \"Após\", \"Com\", \"Contra\", \"De\", \"Desde\", \"Em\", \"Entre\", \"Para\", \"Por\", \"Perante\", \"Sem\", \"Sob\", \"Sobre\", \"Trás\", \"Conforme\", \"Contudo\", \"Durante\", \"Exceto\", \"Mediant\", \"Menos\", \"Salvo\", \"Segundo\", \"Visto\"]\nBRAZIL_PRONOUNS = personal_pronouns + oblique_pronouns + demonstrative_pronouns + possessive_pronouns + indefinite_pronouns + relative_pronouns + interrogative_pronouns + prepositions\nBRAZIL_CONECTORES = ['a', 'o', 'e', 'ou', 'nem', 'mas', 'porque', 'como', 'apesar', 'além', 'entretanto', 'porém', 'todavia', 'logo', 'portanto', 'assim', 'contudo', 'embora', 'ainda', 'também', 'quer', 'seja', 'isto', 'aquilo']\nPRONOUNS = {\n    \"brazil\": BRAZIL_PRONOUNS,",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "possessive_pronouns",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "possessive_pronouns = [\"Meu\", \"Teu\", \"Seu\", \"Nosso\", \"Vosso\", \"Seu\", \"Minha\", \"Tua\", \"Sua\", \"Nossa\", \"Vossa\", \"Sua\", \"Meus\", \"Teus\", \"Seus\", \"Nossos\", \"Vossos\", \"Minhas\", \"Tuas\", \"Suas\", \"Nossas\", \"Vossas\"]\nindefinite_pronouns = [\"Alguém\", \"Ninguém\", \"Todo\", \"Algum\", \"Nenhum\", \"Outro\", \"Muito\", \"Pouco\", \"Tanto\", \"Cada\", \"Algo\", \"Tudo\", \"Nada\", \"Cada um\", \"Qualquer\", \"Poucos\", \"Muitos\", \"Vários\", \"Outrem\"]\nrelative_pronouns = [\"Que\", \"Qual\", \"Quem\", \"Onde\", \"Cujo\", \"O qual\", \"Cuja\", \"Quanto\"]\ninterrogative_pronouns = [\"Quem\", \"O que\", \"Qual\", \"Quanto\", \"Onde\", \"Quando\", \"Como\", \"Por que\", \"Qualquer coisa\", \"Quanto a\"]\nprepositions = [\"A\", \"Ante\", \"Até\", \"Após\", \"Com\", \"Contra\", \"De\", \"Desde\", \"Em\", \"Entre\", \"Para\", \"Por\", \"Perante\", \"Sem\", \"Sob\", \"Sobre\", \"Trás\", \"Conforme\", \"Contudo\", \"Durante\", \"Exceto\", \"Mediant\", \"Menos\", \"Salvo\", \"Segundo\", \"Visto\"]\nBRAZIL_PRONOUNS = personal_pronouns + oblique_pronouns + demonstrative_pronouns + possessive_pronouns + indefinite_pronouns + relative_pronouns + interrogative_pronouns + prepositions\nBRAZIL_CONECTORES = ['a', 'o', 'e', 'ou', 'nem', 'mas', 'porque', 'como', 'apesar', 'além', 'entretanto', 'porém', 'todavia', 'logo', 'portanto', 'assim', 'contudo', 'embora', 'ainda', 'também', 'quer', 'seja', 'isto', 'aquilo']\nPRONOUNS = {\n    \"brazil\": BRAZIL_PRONOUNS,\n}",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "indefinite_pronouns",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "indefinite_pronouns = [\"Alguém\", \"Ninguém\", \"Todo\", \"Algum\", \"Nenhum\", \"Outro\", \"Muito\", \"Pouco\", \"Tanto\", \"Cada\", \"Algo\", \"Tudo\", \"Nada\", \"Cada um\", \"Qualquer\", \"Poucos\", \"Muitos\", \"Vários\", \"Outrem\"]\nrelative_pronouns = [\"Que\", \"Qual\", \"Quem\", \"Onde\", \"Cujo\", \"O qual\", \"Cuja\", \"Quanto\"]\ninterrogative_pronouns = [\"Quem\", \"O que\", \"Qual\", \"Quanto\", \"Onde\", \"Quando\", \"Como\", \"Por que\", \"Qualquer coisa\", \"Quanto a\"]\nprepositions = [\"A\", \"Ante\", \"Até\", \"Após\", \"Com\", \"Contra\", \"De\", \"Desde\", \"Em\", \"Entre\", \"Para\", \"Por\", \"Perante\", \"Sem\", \"Sob\", \"Sobre\", \"Trás\", \"Conforme\", \"Contudo\", \"Durante\", \"Exceto\", \"Mediant\", \"Menos\", \"Salvo\", \"Segundo\", \"Visto\"]\nBRAZIL_PRONOUNS = personal_pronouns + oblique_pronouns + demonstrative_pronouns + possessive_pronouns + indefinite_pronouns + relative_pronouns + interrogative_pronouns + prepositions\nBRAZIL_CONECTORES = ['a', 'o', 'e', 'ou', 'nem', 'mas', 'porque', 'como', 'apesar', 'além', 'entretanto', 'porém', 'todavia', 'logo', 'portanto', 'assim', 'contudo', 'embora', 'ainda', 'também', 'quer', 'seja', 'isto', 'aquilo']\nPRONOUNS = {\n    \"brazil\": BRAZIL_PRONOUNS,\n}\nWORDLIST = {",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "relative_pronouns",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "relative_pronouns = [\"Que\", \"Qual\", \"Quem\", \"Onde\", \"Cujo\", \"O qual\", \"Cuja\", \"Quanto\"]\ninterrogative_pronouns = [\"Quem\", \"O que\", \"Qual\", \"Quanto\", \"Onde\", \"Quando\", \"Como\", \"Por que\", \"Qualquer coisa\", \"Quanto a\"]\nprepositions = [\"A\", \"Ante\", \"Até\", \"Após\", \"Com\", \"Contra\", \"De\", \"Desde\", \"Em\", \"Entre\", \"Para\", \"Por\", \"Perante\", \"Sem\", \"Sob\", \"Sobre\", \"Trás\", \"Conforme\", \"Contudo\", \"Durante\", \"Exceto\", \"Mediant\", \"Menos\", \"Salvo\", \"Segundo\", \"Visto\"]\nBRAZIL_PRONOUNS = personal_pronouns + oblique_pronouns + demonstrative_pronouns + possessive_pronouns + indefinite_pronouns + relative_pronouns + interrogative_pronouns + prepositions\nBRAZIL_CONECTORES = ['a', 'o', 'e', 'ou', 'nem', 'mas', 'porque', 'como', 'apesar', 'além', 'entretanto', 'porém', 'todavia', 'logo', 'portanto', 'assim', 'contudo', 'embora', 'ainda', 'também', 'quer', 'seja', 'isto', 'aquilo']\nPRONOUNS = {\n    \"brazil\": BRAZIL_PRONOUNS,\n}\nWORDLIST = {\n    \"supplement\": SUPPLEMENT_COMPONENT_LIST,",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "interrogative_pronouns",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "interrogative_pronouns = [\"Quem\", \"O que\", \"Qual\", \"Quanto\", \"Onde\", \"Quando\", \"Como\", \"Por que\", \"Qualquer coisa\", \"Quanto a\"]\nprepositions = [\"A\", \"Ante\", \"Até\", \"Após\", \"Com\", \"Contra\", \"De\", \"Desde\", \"Em\", \"Entre\", \"Para\", \"Por\", \"Perante\", \"Sem\", \"Sob\", \"Sobre\", \"Trás\", \"Conforme\", \"Contudo\", \"Durante\", \"Exceto\", \"Mediant\", \"Menos\", \"Salvo\", \"Segundo\", \"Visto\"]\nBRAZIL_PRONOUNS = personal_pronouns + oblique_pronouns + demonstrative_pronouns + possessive_pronouns + indefinite_pronouns + relative_pronouns + interrogative_pronouns + prepositions\nBRAZIL_CONECTORES = ['a', 'o', 'e', 'ou', 'nem', 'mas', 'porque', 'como', 'apesar', 'além', 'entretanto', 'porém', 'todavia', 'logo', 'portanto', 'assim', 'contudo', 'embora', 'ainda', 'também', 'quer', 'seja', 'isto', 'aquilo']\nPRONOUNS = {\n    \"brazil\": BRAZIL_PRONOUNS,\n}\nWORDLIST = {\n    \"supplement\": SUPPLEMENT_COMPONENT_LIST,\n}",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "prepositions",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "prepositions = [\"A\", \"Ante\", \"Até\", \"Após\", \"Com\", \"Contra\", \"De\", \"Desde\", \"Em\", \"Entre\", \"Para\", \"Por\", \"Perante\", \"Sem\", \"Sob\", \"Sobre\", \"Trás\", \"Conforme\", \"Contudo\", \"Durante\", \"Exceto\", \"Mediant\", \"Menos\", \"Salvo\", \"Segundo\", \"Visto\"]\nBRAZIL_PRONOUNS = personal_pronouns + oblique_pronouns + demonstrative_pronouns + possessive_pronouns + indefinite_pronouns + relative_pronouns + interrogative_pronouns + prepositions\nBRAZIL_CONECTORES = ['a', 'o', 'e', 'ou', 'nem', 'mas', 'porque', 'como', 'apesar', 'além', 'entretanto', 'porém', 'todavia', 'logo', 'portanto', 'assim', 'contudo', 'embora', 'ainda', 'também', 'quer', 'seja', 'isto', 'aquilo']\nPRONOUNS = {\n    \"brazil\": BRAZIL_PRONOUNS,\n}\nWORDLIST = {\n    \"supplement\": SUPPLEMENT_COMPONENT_LIST,\n}",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "BRAZIL_PRONOUNS",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "BRAZIL_PRONOUNS = personal_pronouns + oblique_pronouns + demonstrative_pronouns + possessive_pronouns + indefinite_pronouns + relative_pronouns + interrogative_pronouns + prepositions\nBRAZIL_CONECTORES = ['a', 'o', 'e', 'ou', 'nem', 'mas', 'porque', 'como', 'apesar', 'além', 'entretanto', 'porém', 'todavia', 'logo', 'portanto', 'assim', 'contudo', 'embora', 'ainda', 'também', 'quer', 'seja', 'isto', 'aquilo']\nPRONOUNS = {\n    \"brazil\": BRAZIL_PRONOUNS,\n}\nWORDLIST = {\n    \"supplement\": SUPPLEMENT_COMPONENT_LIST,\n}",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "BRAZIL_CONECTORES",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "BRAZIL_CONECTORES = ['a', 'o', 'e', 'ou', 'nem', 'mas', 'porque', 'como', 'apesar', 'além', 'entretanto', 'porém', 'todavia', 'logo', 'portanto', 'assim', 'contudo', 'embora', 'ainda', 'também', 'quer', 'seja', 'isto', 'aquilo']\nPRONOUNS = {\n    \"brazil\": BRAZIL_PRONOUNS,\n}\nWORDLIST = {\n    \"supplement\": SUPPLEMENT_COMPONENT_LIST,\n}",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "PRONOUNS",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "PRONOUNS = {\n    \"brazil\": BRAZIL_PRONOUNS,\n}\nWORDLIST = {\n    \"supplement\": SUPPLEMENT_COMPONENT_LIST,\n}",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "WORDLIST",
        "kind": 5,
        "importPath": "src.lib.wordlist.wordlist",
        "description": "src.lib.wordlist.wordlist",
        "peekOfCode": "WORDLIST = {\n    \"supplement\": SUPPLEMENT_COMPONENT_LIST,\n}",
        "detail": "src.lib.wordlist.wordlist",
        "documentation": {}
    },
    {
        "label": "find_similar_images",
        "kind": 2,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "def find_similar_images(userpaths, hashfunc=imagehash.average_hash):\n\tdef is_image(filename):\n\t\tf = filename.lower()\n\t\treturn f.endswith('.png') or f.endswith('.jpg') or \\\n\t\t\tf.endswith('.jpeg') or f.endswith('.bmp') or \\\n\t\t\tf.endswith('.gif') or '.jpg' in f or f.endswith('.svg')\n\timage_filenames = []\n\tfor userpath in userpaths:\n\t\timage_filenames += [os.path.join(userpath, path) for path in os.listdir(userpath) if is_image(path)]\n\timages = {}",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\tf",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\tf = filename.lower()\n\t\treturn f.endswith('.png') or f.endswith('.jpg') or \\\n\t\t\tf.endswith('.jpeg') or f.endswith('.bmp') or \\\n\t\t\tf.endswith('.gif') or '.jpg' in f or f.endswith('.svg')\n\timage_filenames = []\n\tfor userpath in userpaths:\n\t\timage_filenames += [os.path.join(userpath, path) for path in os.listdir(userpath) if is_image(path)]\n\timages = {}\n\tfor img in sorted(image_filenames):\n\t\ttry:",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\timage_filenames",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\timage_filenames = []\n\tfor userpath in userpaths:\n\t\timage_filenames += [os.path.join(userpath, path) for path in os.listdir(userpath) if is_image(path)]\n\timages = {}\n\tfor img in sorted(image_filenames):\n\t\ttry:\n\t\t\thash = hashfunc(Image.open(img))\n\t\texcept Exception as e:\n\t\t\tprint('Problem:', e, 'with', img)\n\t\t\tcontinue",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\timages",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\timages = {}\n\tfor img in sorted(image_filenames):\n\t\ttry:\n\t\t\thash = hashfunc(Image.open(img))\n\t\texcept Exception as e:\n\t\t\tprint('Problem:', e, 'with', img)\n\t\t\tcontinue\n\t\tif hash in images:\n\t\t\tprint(img, '  already exists as', ' '.join(images[hash]))\n\t\t\tif 'dupPictures' in img:",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\t\thash",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\t\thash = hashfunc(Image.open(img))\n\t\texcept Exception as e:\n\t\t\tprint('Problem:', e, 'with', img)\n\t\t\tcontinue\n\t\tif hash in images:\n\t\t\tprint(img, '  already exists as', ' '.join(images[hash]))\n\t\t\tif 'dupPictures' in img:\n\t\t\t\tprint('rm -v', img)\n\t\timages[hash] = images.get(hash, []) + [img]\n\t# for k, img_list in six.iteritems(images):",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\timages[hash]",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\timages[hash] = images.get(hash, []) + [img]\n\t# for k, img_list in six.iteritems(images):\n\t# \tif len(img_list) > 1:\n\t# \t\tprint(\" \".join(img_list))\nif __name__ == '__main__':  # noqa: C901\n\timport os\n\timport sys\n\tdef usage():\n\t\tsys.stderr.write(\"\"\"SYNOPSIS: %s [ahash|phash|dhash|...] [<directory>]\nIdentifies similar images in the directory.",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\thashmethod",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\thashmethod = sys.argv[1] if len(sys.argv) > 1 else usage()\n\tif hashmethod == 'ahash':\n\t\thashfunc = imagehash.average_hash\n\telif hashmethod == 'phash':\n\t\thashfunc = imagehash.phash\n\telif hashmethod == 'dhash':\n\t\thashfunc = imagehash.dhash\n\telif hashmethod == 'whash-haar':\n\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.average_hash\n\telif hashmethod == 'phash':\n\t\thashfunc = imagehash.phash\n\telif hashmethod == 'dhash':\n\t\thashfunc = imagehash.dhash\n\telif hashmethod == 'whash-haar':\n\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':\n\t\tdef hashfunc(img):\n\t\t\treturn imagehash.whash(img, mode='db4')",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.phash\n\telif hashmethod == 'dhash':\n\t\thashfunc = imagehash.dhash\n\telif hashmethod == 'whash-haar':\n\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':\n\t\tdef hashfunc(img):\n\t\t\treturn imagehash.whash(img, mode='db4')\n\telif hashmethod == 'colorhash':\n\t\thashfunc = imagehash.colorhash",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.dhash\n\telif hashmethod == 'whash-haar':\n\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':\n\t\tdef hashfunc(img):\n\t\t\treturn imagehash.whash(img, mode='db4')\n\telif hashmethod == 'colorhash':\n\t\thashfunc = imagehash.colorhash\n\telif hashmethod == 'crop-resistant':\n\t\thashfunc = imagehash.crop_resistant_hash",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':\n\t\tdef hashfunc(img):\n\t\t\treturn imagehash.whash(img, mode='db4')\n\telif hashmethod == 'colorhash':\n\t\thashfunc = imagehash.colorhash\n\telif hashmethod == 'crop-resistant':\n\t\thashfunc = imagehash.crop_resistant_hash\n\telse:\n\t\tusage()",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.colorhash\n\telif hashmethod == 'crop-resistant':\n\t\thashfunc = imagehash.crop_resistant_hash\n\telse:\n\t\tusage()\n\tuserpaths = sys.argv[2:] if len(sys.argv) > 2 else '.'\n\tfind_similar_images(userpaths=userpaths, hashfunc=hashfunc)",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.crop_resistant_hash\n\telse:\n\t\tusage()\n\tuserpaths = sys.argv[2:] if len(sys.argv) > 2 else '.'\n\tfind_similar_images(userpaths=userpaths, hashfunc=hashfunc)",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\tuserpaths",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\tuserpaths = sys.argv[2:] if len(sys.argv) > 2 else '.'\n\tfind_similar_images(userpaths=userpaths, hashfunc=hashfunc)",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "parse_arguments",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def parse_arguments():\n    \"\"\"\n    Configura e retorna os argumentos da linha de comando.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Processa os argumentos do trabalho.\")\n    parser.add_argument(\"--job_type\", type=str, help=\"\")\n    parser.add_argument(\"--job_name\", type=str, help=\"Nome do trabalho a ser executado.\")\n    parser.add_argument(\"--page_name\", type=str, help=\"Nome da página.\")\n    parser.add_argument(\"--exec_type\", type=str, required=True, choices=[\"extract\", \"transform\", \"load\", \"false\"], help=\"Tipo de trabalho.\")\n    parser.add_argument(\"--exec_flag\", type=str, default=\"\", help=\"Opções adicionais.\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "configure_system",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def configure_system(args):\n    \"\"\"\n    Configura o sistema com base nos argumentos fornecidos.\n    \"\"\"\n    message(\"CONFIGURE SYSTEM\")\n    # Configurações específicas para tipos de trabalho\n    if args.exec_type in [\"extract\", \"transform\"]:\n        configure_display()\n    # Configura o servidor de imagens\n    configure_image_server()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "run_job",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def run_job(args):\n    \"\"\"\n    Importa dinamicamente e executa o módulo de trabalho especificado.\n    Mostra o caminho completo e a stack trace em caso de erro.\n    \"\"\"\n    try:\n        # Define o caminho do módulo do job\n        module_path = f\"src.jobs.{args.job_type}.{args.job_name}.job\"\n        # Importa o módulo dinamicamente\n        job_module = importlib.import_module(module_path)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def main():\n    message(\"START SYSTEM\")\n    # Analisa os argumentos da linha de comando\n    args = parse_arguments()\n    print(args)\n    # Executa o job\n    run_job(args)\nif __name__ == \"__main__\":\n    main()",
        "detail": "main",
        "documentation": {}
    }
]